2019-12-11 15:01:07 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:01:07
2019-12-11 15:01:33 ERROR appraisal:21 â java.util.NoSuchElementException: key not found: learningRate
2019-12-11 15:01:58 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:01:58
2019-12-11 15:02:23 ERROR appraisal:21 â org.apache.spark.sql.AnalysisException: Reference 'originalValue' is ambiguous, could be: originalValue, originalValue.;
2019-12-11 15:03:13 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:03:13
2019-12-11 15:03:25 ERROR appraisal:21 â org.apache.spark.sql.AnalysisException: Reference 'originalValue' is ambiguous, could be: originalValue, originalValue.;
2019-12-11 15:03:39 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:03:39
2019-12-11 15:04:38 ERROR appraisal:21 â org.apache.spark.sql.AnalysisException: Reference 'originalValue' is ambiguous, could be: originalValue, originalValue.;
2019-12-11 15:05:46 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:05:46
2019-12-11 15:07:14 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:07:14
2019-12-11 15:07:52 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:07:52
2019-12-11 15:11:41 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:11:41
2019-12-11 15:12:21 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#992, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#992]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:12:21 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#991, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#991]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:12:21 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:107)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#992, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#992]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:12:21 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 15:13:22 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:13:22
2019-12-11 15:13:40 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#991, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#991]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:13:40 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#992, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#992]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:13:40 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:107)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#991, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#991]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:13:40 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 15:20:15 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:20:15
2019-12-11 15:20:41 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)
	at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:106)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)

2019-12-11 15:20:41 ERROR appraisal:21 â java.lang.NullPointerException: Value at index 0 is null
2019-12-11 15:22:00 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:22:00
2019-12-11 15:22:51 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: Field "features" does not exist.
Available fields: 
	at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)
	at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)
	at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)
	at scala.collection.AbstractMap.getOrElse(Map.scala:59)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:273)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:41)
	at org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)
	at org.apache.spark.ml.PredictionModel.validateAndTransformSchema(Predictor.scala:168)
	at org.apache.spark.ml.PredictionModel.transformSchema(Predictor.scala:192)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)
	at org.apache.spark.ml.PredictionModel.transform(Predictor.scala:203)
	at org.apache.spark.ml.boosting.BoostingParams$class.evaluateOnValidation(BoostingParams.scala:56)
	at org.apache.spark.ml.regression.BoostingRegressor.evaluateOnValidation(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:275)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)
	at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:106)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)

2019-12-11 15:22:51 ERROR appraisal:21 â java.lang.IllegalArgumentException: Field "features" does not exist.
Available fields: 
2019-12-11 15:25:20 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:25:20
2019-12-11 15:25:50 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)
	at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:108)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)

2019-12-11 15:25:50 ERROR appraisal:21 â java.lang.NullPointerException: Value at index 0 is null
2019-12-11 15:29:16 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:29:16
2019-12-11 15:29:43 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)
	at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:110)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)

2019-12-11 15:29:43 ERROR appraisal:21 â java.lang.NullPointerException: Value at index 0 is null
2019-12-11 15:34:41 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:34:41
2019-12-11 15:35:16 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: Field "indexedFeatures" does not exist.
Available fields: features, uniformity_of_cell_size, lineId, label
	at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)
	at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)
	at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)
	at scala.collection.AbstractMap.getOrElse(Map.scala:59)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:273)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:41)
	at org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)
	at org.apache.spark.ml.Predictor.validateAndTransformSchema(Predictor.scala:82)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)
	at org.apache.spark.ml.tuning.ValidatorParams$$anonfun$transformSchemaImpl$2.apply(ValidatorParams.scala:75)
	at org.apache.spark.ml.tuning.ValidatorParams$$anonfun$transformSchemaImpl$2.apply(ValidatorParams.scala:74)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.ValidatorParams$class.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:69)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:186)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:124)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:110)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)

2019-12-11 15:35:16 ERROR appraisal:21 â java.lang.IllegalArgumentException: Field "indexedFeatures" does not exist.
Available fields: features, uniformity_of_cell_size, lineId, label
2019-12-11 15:36:53 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:36:53
2019-12-11 15:37:17 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#992, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#992]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:37:17 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#991, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#991]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:37:17 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:110)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '`val`' given input columns: [label, features];;
'Filter NOT 'val
+- Project [label#991, features#929]
   +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#991]
      +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:37:17 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 15:46:21 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:46:21
2019-12-11 15:46:35 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '(NOT `uniformity_of_cell_size`)' due to data type mismatch: argument 1 requires boolean type, however, '`uniformity_of_cell_size`' is of double type.;;
'Project [label#991, features#929]
+- 'Filter NOT uniformity_of_cell_size#930
   +- Project [label#991, features#929, uniformity_of_cell_size#930]
      +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#991]
         +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:46:35 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '(NOT `uniformity_of_cell_size`)' due to data type mismatch: argument 1 requires boolean type, however, '`uniformity_of_cell_size`' is of double type.;;
'Project [label#992, features#929]
+- 'Filter NOT uniformity_of_cell_size#930
   +- Project [label#992, features#929, uniformity_of_cell_size#930]
      +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#992]
         +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:46:35 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:111)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '(NOT `uniformity_of_cell_size`)' due to data type mismatch: argument 1 requires boolean type, however, '`uniformity_of_cell_size`' is of double type.;;
'Project [label#992, features#929]
+- 'Filter NOT uniformity_of_cell_size#930
   +- Project [label#992, features#929, uniformity_of_cell_size#930]
      +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#992]
         +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:46:36 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 15:47:37 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:47:37
2019-12-11 15:47:38 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 15:47:38
2019-12-11 15:48:01 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '(NOT `uniformity_of_cell_size`)' due to data type mismatch: argument 1 requires boolean type, however, '`uniformity_of_cell_size`' is of double type.;;
'Project [label#991, features#929]
+- 'Filter NOT uniformity_of_cell_size#930
   +- Project [label#991, features#929, uniformity_of_cell_size#930]
      +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#991]
         +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:48:01 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '(NOT `uniformity_of_cell_size`)' due to data type mismatch: argument 1 requires boolean type, however, '`uniformity_of_cell_size`' is of double type.;;
'Project [label#992, features#929]
+- 'Filter NOT uniformity_of_cell_size#930
   +- Project [label#992, features#929, uniformity_of_cell_size#930]
      +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#992]
         +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:48:01 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:111)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '(NOT `uniformity_of_cell_size`)' due to data type mismatch: argument 1 requires boolean type, however, '`uniformity_of_cell_size`' is of double type.;;
'Project [label#991, features#929]
+- 'Filter NOT uniformity_of_cell_size#930
   +- Project [label#991, features#929, uniformity_of_cell_size#930]
      +- Project [features#929, uniformity_of_cell_size#930, lineId#931L, cast(label#932 as double) AS label#991]
         +- LogicalRDD [features#929, uniformity_of_cell_size#930, lineId#931L, label#932], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 15:48:01 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 19:59:08 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 19:59:08
2019-12-11 20:01:22 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 20:01:22
2019-12-11 20:08:20 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 20:08:20
2019-12-11 20:13:33 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 20:13:33
2019-12-11 21:24:20 ERROR appraisal:21 â org.apache.spark.sql.AnalysisException: Union can only be performed on tables with the same number of columns, but the first table has 4 columns and the second table has 5 columns;;
'Union
:- Project [features#888, lineId#841L, label#910, val#915]
:  +- Project [features#888, lineId#841L, originalValue#245, label#910, true AS val#915]
:     +- Project [features#888, lineId#841L, originalValue#245, originalValue#245 AS label#910]
:        +- Project [features#888, lineId#841L, originalValue#245]
:           +- Project [lineId#841L, features#888, originalValue#245]
:              +- Join Inner, (lineId#841L = lineId#32L)
:                 :- Project [features#888, lineId#841L]
:                 :  +- Project [features#888, uniformity_of_cell_size#840, lineId#841L, label#842]
:                 :     +- Project [id#890L, features#888, uniformity_of_cell_size#840, lineId#841L, label#842]
:                 :        +- Join Inner, (id#890L = id#843L)
:                 :           :- Project [features#888, monotonically_increasing_id() AS id#890L]
:                 :           :  +- Project [UDF(features#886) AS features#888]
:                 :           :     +- Project [value#884 AS features#886]
:                 :           :        +- SerializeFromObject [newInstance(class org.apache.spark.sql.catalyst.util.GenericArrayData) AS value#884]
:                 :           :           +- ExternalRDD [obj#883]
:                 :           +- LogicalRDD [uniformity_of_cell_size#840, lineId#841L, label#842, id#843L], false
:                 +- Project [originalValue#245, lineId#32L]
:                    +- Project [lineId#32L, uniformity_of_cell_size#280, clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, bland_chromatin#209, normal_nucleoli#221, mitoses#233, originalValue#245]
:                       +- Join Inner, (lineId#32L = lineId#608L)
:                          :- Project [uniformity_of_cell_size#280, lineId#32L]
:                          :  +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, bare_nuclei#328, bland_chromatin#340, normal_nucleoli#352, mitoses#364, lineId#32L, UDF(originalValue#45) AS originalValue#376]
:                          :     +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, bare_nuclei#328, bland_chromatin#340, normal_nucleoli#352, UDF(mitoses#19) AS mitoses#364, lineId#32L, originalValue#45]
:                          :        +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, bare_nuclei#328, bland_chromatin#340, UDF(normal_nucleoli#18) AS normal_nucleoli#352, mitoses#19, lineId#32L, originalValue#45]
:                          :           +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, bare_nuclei#328, UDF(bland_chromatin#17) AS bland_chromatin#340, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :              +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, UDF(bare_nuclei#16) AS bare_nuclei#328, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                 +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, UDF(single_epithelial_cell_size#15) AS single_epithelial_cell_size#316, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                    +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, UDF(marginal_adhesion#14) AS marginal_adhesion#304, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                       +- Project [clump_thickness#268, uniformity_of_cell_size#280, UDF(uniformity_of_cell_shape#13) AS uniformity_of_cell_shape#292, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                          +- Project [clump_thickness#268, UDF(uniformity_of_cell_size#12) AS uniformity_of_cell_size#280, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                             +- Project [UDF(clump_thickness#11) AS clump_thickness#268, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                                +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                   +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                      +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                         +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                            +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                               +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                                  +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                                     +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                                        +- Project [clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                                                           +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#32L, uniformity_of_cell_size#12 AS originalValue#45]
:                          :                                                              +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#32L]
:                          :                                                                 +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
:                          +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, bland_chromatin#209, normal_nucleoli#221, mitoses#233, lineId#608L, UDF(originalValue#45) AS originalValue#245]
:                             +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, bland_chromatin#209, normal_nucleoli#221, UDF(mitoses#19) AS mitoses#233, lineId#608L, originalValue#45]
:                                +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, bland_chromatin#209, UDF(normal_nucleoli#18) AS normal_nucleoli#221, mitoses#19, lineId#608L, originalValue#45]
:                                   +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, UDF(bland_chromatin#17) AS bland_chromatin#209, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                      +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, UDF(bare_nuclei#16) AS bare_nuclei#197, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                         +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, UDF(single_epithelial_cell_size#15) AS single_epithelial_cell_size#185, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                            +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, UDF(marginal_adhesion#14) AS marginal_adhesion#173, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                               +- Project [clump_thickness#137, uniformity_of_cell_size#149, UDF(uniformity_of_cell_shape#13) AS uniformity_of_cell_shape#161, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                                  +- Project [clump_thickness#137, UDF(uniformity_of_cell_size#98) AS uniformity_of_cell_size#149, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                                     +- Project [UDF(clump_thickness#11) AS clump_thickness#137, uniformity_of_cell_size#98, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                                        +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                           +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                              +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                 +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                    +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                       +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                          +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                             +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                                +- Project [clump_thickness#11, uniformity_of_cell_size#98, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                                                                   +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#98, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#608L, originalValue#45]
:                                                                                      +- Project [code_number#10, clump_thickness#11, ncolumn#79 AS uniformity_of_cell_size#98, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#608L, originalValue#45, ncolumn#79]
:                                                                                         +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#608L, originalValue#45, ncolumn#79]
:                                                                                            +- Filter (lineId#608L = lineId#83L)
:                                                                                               +- Join Inner
:                                                                                                  :- SubqueryAlias `o`
:                                                                                                  :  +- SubqueryAlias `originaldb`
:                                                                                                  :     +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#608L, uniformity_of_cell_size#12 AS originalValue#45]
:                                                                                                  :        +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#608L]
:                                                                                                  :           +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
:                                                                                                  +- SubqueryAlias `n`
:                                                                                                     +- SubqueryAlias `ncoldf`
:                                                                                                        +- Project [lineId#83L, CASE WHEN (lineId#83L = lineId#80L) THEN cast(null as string) ELSE uniformity_of_cell_size#12 END AS ncolumn#79]
:                                                                                                           +- Join LeftOuter, (lineId#83L = lineId#80L)
:                                                                                                              :- SubqueryAlias `o`
:                                                                                                              :  +- SubqueryAlias `originaldb`
:                                                                                                              :     +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#83L, uniformity_of_cell_size#12 AS originalValue#45]
:                                                                                                              :        +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#83L]
:                                                                                                              :           +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
:                                                                                                              +- SubqueryAlias `n`
:                                                                                                                 +- SubqueryAlias `niddf`
:                                                                                                                    +- GlobalLimit 14
:                                                                                                                       +- LocalLimit 14
:                                                                                                                          +- Project [lineId#80L]
:                                                                                                                             +- Sort [_nondeterministic#77 ASC NULLS FIRST], true
:                                                                                                                                +- Project [lineId#80L, rand(3048107458041612002) AS _nondeterministic#77]
:                                                                                                                                   +- Project [lineId#80L]
:                                                                                                                                      +- SubqueryAlias `originaldb`
:                                                                                                                                         +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#80L, uniformity_of_cell_size#12 AS originalValue#45]
:                                                                                                                                            +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#80L]
:                                                                                                                                               +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
+- Project [features#771, uniformity_of_cell_size#723, lineId#724L, label#725, false AS val#785]
   +- Project [features#771, uniformity_of_cell_size#723, lineId#724L, label#725]
      +- Project [id#773L, features#771, uniformity_of_cell_size#723, lineId#724L, label#725]
         +- Join Inner, (id#773L = id#726L)
            :- Project [features#771, monotonically_increasing_id() AS id#773L]
            :  +- Project [UDF(features#769) AS features#771]
            :     +- Project [value#767 AS features#769]
            :        +- SerializeFromObject [newInstance(class org.apache.spark.sql.catalyst.util.GenericArrayData) AS value#767]
            :           +- ExternalRDD [obj#766]
            +- LogicalRDD [uniformity_of_cell_size#723, lineId#724L, label#725, id#726L], false

2019-12-11 21:36:24 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 21:36:24
2019-12-11 21:36:42 ERROR appraisal:21 â org.apache.spark.sql.AnalysisException: Union can only be performed on tables with the same number of columns, but the first table has 4 columns and the second table has 5 columns;;
'Union
:- Project [features#888, lineId#841L, label#910, val#915]
:  +- Project [features#888, lineId#841L, originalValue#245, label#910, true AS val#915]
:     +- Project [features#888, lineId#841L, originalValue#245, originalValue#245 AS label#910]
:        +- Project [features#888, lineId#841L, originalValue#245]
:           +- Project [lineId#841L, features#888, originalValue#245]
:              +- Join Inner, (lineId#841L = lineId#32L)
:                 :- Project [features#888, lineId#841L]
:                 :  +- Project [features#888, uniformity_of_cell_size#840, lineId#841L, label#842]
:                 :     +- Project [id#890L, features#888, uniformity_of_cell_size#840, lineId#841L, label#842]
:                 :        +- Join Inner, (id#890L = id#843L)
:                 :           :- Project [features#888, monotonically_increasing_id() AS id#890L]
:                 :           :  +- Project [UDF(features#886) AS features#888]
:                 :           :     +- Project [value#884 AS features#886]
:                 :           :        +- SerializeFromObject [newInstance(class org.apache.spark.sql.catalyst.util.GenericArrayData) AS value#884]
:                 :           :           +- ExternalRDD [obj#883]
:                 :           +- LogicalRDD [uniformity_of_cell_size#840, lineId#841L, label#842, id#843L], false
:                 +- Project [originalValue#245, lineId#32L]
:                    +- Project [lineId#32L, uniformity_of_cell_size#280, clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, bland_chromatin#209, normal_nucleoli#221, mitoses#233, originalValue#245]
:                       +- Join Inner, (lineId#32L = lineId#608L)
:                          :- Project [uniformity_of_cell_size#280, lineId#32L]
:                          :  +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, bare_nuclei#328, bland_chromatin#340, normal_nucleoli#352, mitoses#364, lineId#32L, UDF(originalValue#45) AS originalValue#376]
:                          :     +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, bare_nuclei#328, bland_chromatin#340, normal_nucleoli#352, UDF(mitoses#19) AS mitoses#364, lineId#32L, originalValue#45]
:                          :        +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, bare_nuclei#328, bland_chromatin#340, UDF(normal_nucleoli#18) AS normal_nucleoli#352, mitoses#19, lineId#32L, originalValue#45]
:                          :           +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, bare_nuclei#328, UDF(bland_chromatin#17) AS bland_chromatin#340, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :              +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, single_epithelial_cell_size#316, UDF(bare_nuclei#16) AS bare_nuclei#328, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                 +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, marginal_adhesion#304, UDF(single_epithelial_cell_size#15) AS single_epithelial_cell_size#316, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                    +- Project [clump_thickness#268, uniformity_of_cell_size#280, uniformity_of_cell_shape#292, UDF(marginal_adhesion#14) AS marginal_adhesion#304, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                       +- Project [clump_thickness#268, uniformity_of_cell_size#280, UDF(uniformity_of_cell_shape#13) AS uniformity_of_cell_shape#292, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                          +- Project [clump_thickness#268, UDF(uniformity_of_cell_size#12) AS uniformity_of_cell_size#280, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                             +- Project [UDF(clump_thickness#11) AS clump_thickness#268, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                                +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                   +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                      +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                         +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                            +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                               +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                                  +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                                     +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#12.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                          :                                                        +- Project [clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L, originalValue#45]
:                          :                                                           +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#32L, uniformity_of_cell_size#12 AS originalValue#45]
:                          :                                                              +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#32L]
:                          :                                                                 +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
:                          +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, bland_chromatin#209, normal_nucleoli#221, mitoses#233, lineId#608L, UDF(originalValue#45) AS originalValue#245]
:                             +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, bland_chromatin#209, normal_nucleoli#221, UDF(mitoses#19) AS mitoses#233, lineId#608L, originalValue#45]
:                                +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, bland_chromatin#209, UDF(normal_nucleoli#18) AS normal_nucleoli#221, mitoses#19, lineId#608L, originalValue#45]
:                                   +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, bare_nuclei#197, UDF(bland_chromatin#17) AS bland_chromatin#209, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                      +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, single_epithelial_cell_size#185, UDF(bare_nuclei#16) AS bare_nuclei#197, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                         +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, marginal_adhesion#173, UDF(single_epithelial_cell_size#15) AS single_epithelial_cell_size#185, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                            +- Project [clump_thickness#137, uniformity_of_cell_size#149, uniformity_of_cell_shape#161, UDF(marginal_adhesion#14) AS marginal_adhesion#173, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                               +- Project [clump_thickness#137, uniformity_of_cell_size#149, UDF(uniformity_of_cell_shape#13) AS uniformity_of_cell_shape#161, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                                  +- Project [clump_thickness#137, UDF(uniformity_of_cell_size#98) AS uniformity_of_cell_size#149, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                                     +- Project [UDF(clump_thickness#11) AS clump_thickness#137, uniformity_of_cell_size#98, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                                        +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                           +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                              +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                 +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                    +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                       +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                          +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                             +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#98.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#608L, originalValue#45.toString, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false), StructField(originalValue,StringType,true))
:                                                                                +- Project [clump_thickness#11, uniformity_of_cell_size#98, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#608L, originalValue#45]
:                                                                                   +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#98, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#608L, originalValue#45]
:                                                                                      +- Project [code_number#10, clump_thickness#11, ncolumn#79 AS uniformity_of_cell_size#98, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#608L, originalValue#45, ncolumn#79]
:                                                                                         +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#608L, originalValue#45, ncolumn#79]
:                                                                                            +- Filter (lineId#608L = lineId#83L)
:                                                                                               +- Join Inner
:                                                                                                  :- SubqueryAlias `o`
:                                                                                                  :  +- SubqueryAlias `originaldb`
:                                                                                                  :     +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#608L, uniformity_of_cell_size#12 AS originalValue#45]
:                                                                                                  :        +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#608L]
:                                                                                                  :           +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
:                                                                                                  +- SubqueryAlias `n`
:                                                                                                     +- SubqueryAlias `ncoldf`
:                                                                                                        +- Project [lineId#83L, CASE WHEN (lineId#83L = lineId#80L) THEN cast(null as string) ELSE uniformity_of_cell_size#12 END AS ncolumn#79]
:                                                                                                           +- Join LeftOuter, (lineId#83L = lineId#80L)
:                                                                                                              :- SubqueryAlias `o`
:                                                                                                              :  +- SubqueryAlias `originaldb`
:                                                                                                              :     +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#83L, uniformity_of_cell_size#12 AS originalValue#45]
:                                                                                                              :        +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#83L]
:                                                                                                              :           +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
:                                                                                                              +- SubqueryAlias `n`
:                                                                                                                 +- SubqueryAlias `niddf`
:                                                                                                                    +- GlobalLimit 14
:                                                                                                                       +- LocalLimit 14
:                                                                                                                          +- Project [lineId#80L]
:                                                                                                                             +- Sort [_nondeterministic#77 ASC NULLS FIRST], true
:                                                                                                                                +- Project [lineId#80L, rand(-371169838117967287) AS _nondeterministic#77]
:                                                                                                                                   +- Project [lineId#80L]
:                                                                                                                                      +- SubqueryAlias `originaldb`
:                                                                                                                                         +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#80L, uniformity_of_cell_size#12 AS originalValue#45]
:                                                                                                                                            +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#80L]
:                                                                                                                                               +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
+- Project [features#771, uniformity_of_cell_size#723, lineId#724L, label#725, false AS val#785]
   +- Project [features#771, uniformity_of_cell_size#723, lineId#724L, label#725]
      +- Project [id#773L, features#771, uniformity_of_cell_size#723, lineId#724L, label#725]
         +- Join Inner, (id#773L = id#726L)
            :- Project [features#771, monotonically_increasing_id() AS id#773L]
            :  +- Project [UDF(features#769) AS features#771]
            :     +- Project [value#767 AS features#769]
            :        +- SerializeFromObject [newInstance(class org.apache.spark.sql.catalyst.util.GenericArrayData) AS value#767]
            :           +- ExternalRDD [obj#766]
            +- LogicalRDD [uniformity_of_cell_size#723, lineId#724L, label#725, id#726L], false

2019-12-11 21:37:52 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 21:37:52
2019-12-11 21:38:05 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`uniformity_of_cell_size`' given input columns: [label, features];;
'Filter NOT 'uniformity_of_cell_size
+- Project [label#1137, features#1074]
   +- Project [features#1074, lineId#1075L, cast(label#1076 as double) AS label#1137, val#1077]
      +- LogicalRDD [features#1074, lineId#1075L, label#1076, val#1077], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 21:38:05 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`uniformity_of_cell_size`' given input columns: [label, features];;
'Filter NOT 'uniformity_of_cell_size
+- Project [label#1136, features#1074]
   +- Project [features#1074, lineId#1075L, cast(label#1076 as double) AS label#1136, val#1077]
      +- LogicalRDD [features#1074, lineId#1075L, label#1076, val#1077], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 21:38:05 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:111)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '`uniformity_of_cell_size`' given input columns: [label, features];;
'Filter NOT 'uniformity_of_cell_size
+- Project [label#1137, features#1074]
   +- Project [features#1074, lineId#1075L, cast(label#1076 as double) AS label#1137, val#1077]
      +- LogicalRDD [features#1074, lineId#1075L, label#1076, val#1077], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 21:38:05 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 21:39:28 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 21:39:28
2019-12-11 21:40:39 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 21:40:39
2019-12-11 21:44:20 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 21:44:20
2019-12-11 21:44:50 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`uniformity_of_cell_size`' given input columns: [label, features];;
'Filter NOT 'uniformity_of_cell_size
+- Project [label#1154, features#1092]
   +- Project [features#1092, lineId#1093L, cast(label#1094 as double) AS label#1154, val#1095]
      +- LogicalRDD [features#1092, lineId#1093L, label#1094, val#1095], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 21:44:50 ERROR Instrumentation:70 â org.apache.spark.sql.AnalysisException: cannot resolve '`uniformity_of_cell_size`' given input columns: [label, features];;
'Filter NOT 'uniformity_of_cell_size
+- Project [label#1155, features#1092]
   +- Project [features#1092, lineId#1093L, cast(label#1094 as double) AS label#1155, val#1095]
      +- LogicalRDD [features#1092, lineId#1093L, label#1094, val#1095], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 21:44:50 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:108)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '`uniformity_of_cell_size`' given input columns: [label, features];;
'Filter NOT 'uniformity_of_cell_size
+- Project [label#1154, features#1092]
   +- Project [features#1092, lineId#1093L, cast(label#1094 as double) AS label#1154, val#1095]
      +- LogicalRDD [features#1092, lineId#1093L, label#1094, val#1095], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:183)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 21:44:50 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 21:50:04 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 21:50:04
2019-12-11 21:51:21 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 21:51:30 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:108)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 21:51:30 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 22:18:53 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 22:18:53
2019-12-11 22:19:55 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:19:55 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:108)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:19:55 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 22:19:55 ERROR Instrumentation:70 â org.apache.spark.SparkException: Job 80 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:240)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:19:55 ERROR Instrumentation:70 â org.apache.spark.SparkException: Job 80 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:240)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:19:55 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@34d43bf0 rejected from java.util.concurrent.ThreadPoolExecutor@17a8cdad[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19580]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@3983794a rejected from java.util.concurrent.ThreadPoolExecutor@7d7ad810[Shutting down, pool size = 2, active threads = 2, queued tasks = 0, completed tasks = 19586]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@de09972 rejected from java.util.concurrent.ThreadPoolExecutor@17a8cdad[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19580]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@101100ff rejected from java.util.concurrent.ThreadPoolExecutor@7d7ad810[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19588]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@5a57ac57 rejected from java.util.concurrent.ThreadPoolExecutor@17a8cdad[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19580]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@416e9aa rejected from java.util.concurrent.ThreadPoolExecutor@7d7ad810[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19588]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@10be2325 rejected from java.util.concurrent.ThreadPoolExecutor@17a8cdad[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19580]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@7c007616 rejected from java.util.concurrent.ThreadPoolExecutor@7d7ad810[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19588]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@2b1fe936 rejected from java.util.concurrent.ThreadPoolExecutor@17a8cdad[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19580]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@2c7c5bf3 rejected from java.util.concurrent.ThreadPoolExecutor@7d7ad810[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19588]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@2d3566cb rejected from java.util.concurrent.ThreadPoolExecutor@17a8cdad[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19580]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@df0d431 rejected from java.util.concurrent.ThreadPoolExecutor@7d7ad810[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19588]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@118905c rejected from java.util.concurrent.ThreadPoolExecutor@17a8cdad[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19580]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6cb31cf4 rejected from java.util.concurrent.ThreadPoolExecutor@7d7ad810[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19588]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@4227d8ea rejected from java.util.concurrent.ThreadPoolExecutor@17a8cdad[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19580]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:19:55 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@7682f8ae rejected from java.util.concurrent.ThreadPoolExecutor@7d7ad810[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 19588]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:26:24 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 22:26:24
2019-12-11 22:32:39 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 22:32:39
2019-12-11 22:33:20 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:33:20 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:108)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:33:20 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 22:33:20 ERROR Instrumentation:70 â org.apache.spark.SparkException: Job 53 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2557)
	at org.apache.spark.sql.Dataset.first(Dataset.scala:2564)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:102)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:33:20 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@6977d2af rejected from java.util.concurrent.ThreadPoolExecutor@5f6ce793[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@4a62c23 rejected from java.util.concurrent.ThreadPoolExecutor@a6ad389[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11954]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@72dee186 rejected from java.util.concurrent.ThreadPoolExecutor@5f6ce793[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@74d143e4 rejected from java.util.concurrent.ThreadPoolExecutor@a6ad389[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11954]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@4d13906c rejected from java.util.concurrent.ThreadPoolExecutor@5f6ce793[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6807c6e7 rejected from java.util.concurrent.ThreadPoolExecutor@a6ad389[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11954]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@74c741ed rejected from java.util.concurrent.ThreadPoolExecutor@5f6ce793[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@2e8fe66d rejected from java.util.concurrent.ThreadPoolExecutor@a6ad389[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11954]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@50bba603 rejected from java.util.concurrent.ThreadPoolExecutor@5f6ce793[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@3b7ba521 rejected from java.util.concurrent.ThreadPoolExecutor@a6ad389[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11954]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@6f43a7b rejected from java.util.concurrent.ThreadPoolExecutor@5f6ce793[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@58d80a50 rejected from java.util.concurrent.ThreadPoolExecutor@a6ad389[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11954]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@4b7ac471 rejected from java.util.concurrent.ThreadPoolExecutor@5f6ce793[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@4f742ced rejected from java.util.concurrent.ThreadPoolExecutor@a6ad389[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11954]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@267ffcf8 rejected from java.util.concurrent.ThreadPoolExecutor@5f6ce793[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:33:20 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@5524e987 rejected from java.util.concurrent.ThreadPoolExecutor@a6ad389[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11954]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:42:06 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 22:42:06
2019-12-11 22:43:54 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:43:54 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:108)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:43:54 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 22:43:54 ERROR Instrumentation:70 â org.apache.spark.SparkException: Job 165 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:240)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:43:54 ERROR Instrumentation:70 â org.apache.spark.SparkException: Job 165 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:240)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:43:54 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@3b6717cd rejected from java.util.concurrent.ThreadPoolExecutor@525732db[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57328]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@621474f4 rejected from java.util.concurrent.ThreadPoolExecutor@28a86634[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 57335]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@e12ed52 rejected from java.util.concurrent.ThreadPoolExecutor@525732db[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57328]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@26fb61a2 rejected from java.util.concurrent.ThreadPoolExecutor@28a86634[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57336]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@4fc93336 rejected from java.util.concurrent.ThreadPoolExecutor@525732db[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57328]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@a6a81d3 rejected from java.util.concurrent.ThreadPoolExecutor@28a86634[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57336]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@4c3944c8 rejected from java.util.concurrent.ThreadPoolExecutor@525732db[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57328]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@2282f922 rejected from java.util.concurrent.ThreadPoolExecutor@28a86634[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57336]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@37c93b90 rejected from java.util.concurrent.ThreadPoolExecutor@525732db[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57328]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@2ef89837 rejected from java.util.concurrent.ThreadPoolExecutor@28a86634[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57336]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@40b1178a rejected from java.util.concurrent.ThreadPoolExecutor@525732db[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57328]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6351af69 rejected from java.util.concurrent.ThreadPoolExecutor@28a86634[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57336]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@1e8348d8 rejected from java.util.concurrent.ThreadPoolExecutor@525732db[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57328]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6c305c97 rejected from java.util.concurrent.ThreadPoolExecutor@28a86634[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57336]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@61a6d1a7 rejected from java.util.concurrent.ThreadPoolExecutor@525732db[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57328]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:43:54 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@7aa047fb rejected from java.util.concurrent.ThreadPoolExecutor@28a86634[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 57336]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 22:57:27 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 22:57:27
2019-12-11 22:58:25 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 22:58:25
2019-12-11 22:59:26 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:59:36 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:113)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 22:59:36 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 23:06:08 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:06:08
2019-12-11 23:07:15 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 23:08:00 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 23:08:00 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:108)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 23:08:00 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 23:11:00 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:11:00
2019-12-11 23:13:27 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:13:27
2019-12-11 23:15:08 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:15:08
2019-12-11 23:17:26 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:17:26
2019-12-11 23:17:37 ERROR Executor:91 â Exception in task 22.0 in stage 56.0 (TID 281)
java.lang.ClassCastException: org.apache.spark.ml.linalg.DenseVector cannot be cast to [D
	at appraisal.spark.algorithm.AdaboostR2$$anonfun$run$1.apply(AdaboostR2.scala:83)
	at appraisal.spark.algorithm.AdaboostR2$$anonfun$run$1.apply(AdaboostR2.scala:83)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:927)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:927)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-11 23:17:37 ERROR TaskSetManager:70 â Task 22 in stage 56.0 failed 1 times; aborting job
2019-12-11 23:17:37 ERROR appraisal:21 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 56.0 failed 1 times, most recent failure: Lost task 22.0 in stage 56.0 (TID 281, localhost, executor driver): java.lang.ClassCastException: org.apache.spark.ml.linalg.DenseVector cannot be cast to [D
	at appraisal.spark.algorithm.AdaboostR2$$anonfun$run$1.apply(AdaboostR2.scala:83)
	at appraisal.spark.algorithm.AdaboostR2$$anonfun$run$1.apply(AdaboostR2.scala:83)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:927)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:927)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-11 23:18:35 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:18:35
2019-12-11 23:19:50 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:19:50
2019-12-11 23:22:19 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:22:19
2019-12-11 23:24:04 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:24:04
2019-12-11 23:25:05 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:25:05
2019-12-11 23:28:37 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:28:37
2019-12-11 23:29:36 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 23:29:40 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 23:29:40 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:107)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 23:29:40 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
2019-12-11 23:31:09 ERROR AdaboostR2Exec$:19 â Appraisal Spark - Wall start time: 11/12/2019 23:31:09
2019-12-11 23:32:47 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 23:32:47 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:108)
	at appraisal.spark.executor.poc.AdaboostR2Exec$.main(AdaboostR2Exec.scala:89)
	at appraisal.spark.executor.poc.AdaboostR2Exec.main(AdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.avgLoss(BoostingParams.scala:103)
	at org.apache.spark.ml.regression.BoostingRegressor.avgLoss(BoostingRegressor.scala:116)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:253)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:330)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:153)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:152)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:116)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-12-11 23:32:47 ERROR appraisal:21 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
