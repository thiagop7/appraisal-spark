2019-12-06 11:22:26 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 11:22:26
2019-12-06 11:23:11 ERROR appraisal:54 â java.lang.IllegalArgumentException: Field "features" does not exist.
Available fields: clump_thickness, uniformity_of_cell_size, uniformity_of_cell_shape, marginal_adhesion, single_epithelial_cell_size, bare_nuclei, bland_chromatin, normal_nucleoli, mitoses
2019-12-06 11:24:39 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 11:24:39
2019-12-06 11:29:55 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 11:29:55
2019-12-06 11:30:39 ERROR appraisal:54 â java.lang.IllegalArgumentException: Field "features" does not exist.
Available fields: clump_thickness, uniformity_of_cell_size, uniformity_of_cell_shape, marginal_adhesion, single_epithelial_cell_size, bare_nuclei, bland_chromatin, normal_nucleoli, mitoses, lineId
2019-12-06 11:38:14 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 11:38:14
2019-12-06 11:38:26 ERROR Executor:91 â Exception in task 0.0 in stage 35.0 (TID 52)
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-06 11:38:26 ERROR TaskSetManager:70 â Task 0 in stage 35.0 failed 1 times; aborting job
2019-12-06 11:38:26 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 52, localhost, executor driver): java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-06 11:54:18 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 11:54:18
2019-12-06 11:55:42 ERROR Executor:91 â Exception in task 0.0 in stage 35.0 (TID 52)
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-06 11:55:42 ERROR TaskSetManager:70 â Task 0 in stage 35.0 failed 1 times; aborting job
2019-12-06 11:55:42 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 52, localhost, executor driver): java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-06 11:59:02 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 11:59:02
2019-12-06 11:59:17 ERROR Executor:91 â Exception in task 0.0 in stage 21.0 (TID 21)
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-06 11:59:17 ERROR TaskSetManager:70 â Task 0 in stage 21.0 failed 1 times; aborting job
2019-12-06 11:59:17 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost, executor driver): java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-06 12:00:32 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:00:32
2019-12-06 12:01:46 ERROR Executor:91 â Exception in task 0.0 in stage 21.0 (TID 21)
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-06 12:01:46 ERROR TaskSetManager:70 â Task 0 in stage 21.0 failed 1 times; aborting job
2019-12-06 12:01:46 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost, executor driver): java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-06 12:02:46 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:02:46
2019-12-06 12:05:54 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:05:54
2019-12-06 12:06:20 ERROR Executor:91 â Exception in task 0.0 in stage 21.0 (TID 21)
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:44)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-06 12:06:20 ERROR TaskSetManager:70 â Task 0 in stage 21.0 failed 1 times; aborting job
2019-12-06 12:06:20 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost, executor driver): java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:44)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-06 12:10:53 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:10:53
2019-12-06 12:11:42 ERROR Executor:91 â Exception in task 0.0 in stage 21.0 (TID 21)
scala.MatchError: 0 (of class java.lang.Long)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:117)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:117)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:44)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-06 12:11:42 ERROR TaskSetManager:70 â Task 0 in stage 21.0 failed 1 times; aborting job
2019-12-06 12:11:42 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost, executor driver): scala.MatchError: 0 (of class java.lang.Long)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:117)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:117)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:44)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-06 12:12:47 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:12:47
2019-12-06 12:13:09 ERROR Executor:91 â Exception in task 0.0 in stage 20.0 (TID 20)
scala.MatchError: 0 (of class java.lang.Long)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:116)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:116)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:44)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-06 12:13:09 ERROR TaskSetManager:70 â Task 0 in stage 20.0 failed 1 times; aborting job
2019-12-06 12:13:09 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 20, localhost, executor driver): scala.MatchError: 0 (of class java.lang.Long)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:116)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:116)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:44)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-06 12:13:25 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:13:25
2019-12-06 12:14:04 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:14:04
2019-12-06 12:15:22 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:15:22
2019-12-06 12:16:45 ERROR Executor:91 â Exception in task 0.0 in stage 32.0 (TID 54)
scala.MatchError: null
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:116)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:116)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:44)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-12-06 12:16:45 ERROR TaskSetManager:70 â Task 0 in stage 32.0 failed 1 times; aborting job
2019-12-06 12:16:45 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 54, localhost, executor driver): scala.MatchError: null
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:116)
	at appraisal.spark.util.Util$$anonfun$extractDouble$1.apply(Util.scala:116)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:44)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3$$anonfun$apply$1.apply(RandomForest.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:42)
	at appraisal.spark.algorithm.RandomForest$$anonfun$3.apply(RandomForest.scala:39)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-12-06 12:19:37 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:19:37
2019-12-06 12:22:01 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:22:01
2019-12-06 12:22:59 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:22:59
2019-12-06 12:23:18 ERROR appraisal:54 â org.apache.spark.sql.AnalysisException: cannot resolve '`uniformity_of_cell_size`' given input columns: [bare_nuclei, normal_nucleoli, single_epithelial_cell_size, mitoses, o.lineId, bland_chromatin, marginal_adhesion, clump_thickness, uniformity_of_cell_shape];;
'Project ['uniformity_of_cell_size]
+- Project [clump_thickness#118, uniformity_of_cell_shape#140, marginal_adhesion#151, single_epithelial_cell_size#162, bare_nuclei#173, bland_chromatin#184, normal_nucleoli#195, mitoses#206, lineId#32L]
   +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,DoubleType,true), StructField(uniformity_of_cell_size,DoubleType,true), StructField(uniformity_of_cell_shape,DoubleType,true), StructField(marginal_adhesion,DoubleType,true), StructField(single_epithelial_cell_size,DoubleType,true), StructField(bare_nuclei,DoubleType,true), StructField(bland_chromatin,DoubleType,true), StructField(normal_nucleoli,DoubleType,true), StructField(mitoses,DoubleType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#118, uniformity_of_cell_size#129, uniformity_of_cell_shape#140, marginal_adhesion#151, single_epithelial_cell_size#162, bare_nuclei#173, bland_chromatin#184, normal_nucleoli#195, mitoses#206, lineId#32L, StructField(clump_thickness,DoubleType,true), StructField(uniformity_of_cell_size,DoubleType,true), StructField(uniformity_of_cell_shape,DoubleType,true), StructField(marginal_adhesion,DoubleType,true), StructField(single_epithelial_cell_size,DoubleType,true), StructField(bare_nuclei,DoubleType,true), StructField(bland_chromatin,DoubleType,true), StructField(normal_nucleoli,DoubleType,true), StructField(mitoses,DoubleType,true), StructField(lineId,LongType,false))
      +- Project [clump_thickness#118, uniformity_of_cell_size#129, uniformity_of_cell_shape#140, marginal_adhesion#151, single_epithelial_cell_size#162, bare_nuclei#173, bland_chromatin#184, normal_nucleoli#195, UDF(mitoses#19) AS mitoses#206, lineId#32L]
         +- Project [clump_thickness#118, uniformity_of_cell_size#129, uniformity_of_cell_shape#140, marginal_adhesion#151, single_epithelial_cell_size#162, bare_nuclei#173, bland_chromatin#184, UDF(normal_nucleoli#18) AS normal_nucleoli#195, mitoses#19, lineId#32L]
            +- Project [clump_thickness#118, uniformity_of_cell_size#129, uniformity_of_cell_shape#140, marginal_adhesion#151, single_epithelial_cell_size#162, bare_nuclei#173, UDF(bland_chromatin#17) AS bland_chromatin#184, normal_nucleoli#18, mitoses#19, lineId#32L]
               +- Project [clump_thickness#118, uniformity_of_cell_size#129, uniformity_of_cell_shape#140, marginal_adhesion#151, single_epithelial_cell_size#162, UDF(bare_nuclei#16) AS bare_nuclei#173, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L]
                  +- Project [clump_thickness#118, uniformity_of_cell_size#129, uniformity_of_cell_shape#140, marginal_adhesion#151, UDF(single_epithelial_cell_size#15) AS single_epithelial_cell_size#162, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L]
                     +- Project [clump_thickness#118, uniformity_of_cell_size#129, uniformity_of_cell_shape#140, UDF(marginal_adhesion#14) AS marginal_adhesion#151, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L]
                        +- Project [clump_thickness#118, uniformity_of_cell_size#129, UDF(uniformity_of_cell_shape#13) AS uniformity_of_cell_shape#140, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L]
                           +- Project [clump_thickness#118, UDF(uniformity_of_cell_size#82) AS uniformity_of_cell_size#129, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L]
                              +- Project [UDF(clump_thickness#11) AS clump_thickness#118, uniformity_of_cell_size#82, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L]
                                 +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#82.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false))
                                    +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#82.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false))
                                       +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#82.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false))
                                          +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#82.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false))
                                             +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#82.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false))
                                                +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#82.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false))
                                                   +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#82.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false))
                                                      +- TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false)], createexternalrow(clump_thickness#11.toString, uniformity_of_cell_size#82.toString, uniformity_of_cell_shape#13.toString, marginal_adhesion#14.toString, single_epithelial_cell_size#15.toString, bare_nuclei#16.toString, bland_chromatin#17.toString, normal_nucleoli#18.toString, mitoses#19.toString, lineId#32L, StructField(clump_thickness,StringType,true), StructField(uniformity_of_cell_size,StringType,true), StructField(uniformity_of_cell_shape,StringType,true), StructField(marginal_adhesion,StringType,true), StructField(single_epithelial_cell_size,StringType,true), StructField(bare_nuclei,StringType,true), StructField(bland_chromatin,StringType,true), StructField(normal_nucleoli,StringType,true), StructField(mitoses,StringType,true), StructField(lineId,LongType,false))
                                                         +- Project [clump_thickness#11, uniformity_of_cell_size#82, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, lineId#32L]
                                                            +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#82, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#32L]
                                                               +- Project [code_number#10, clump_thickness#11, ncolumn#64 AS uniformity_of_cell_size#82, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#32L, ncolumn#64]
                                                                  +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, lineId#32L, ncolumn#64]
                                                                     +- Filter (lineId#32L = lineId#68L)
                                                                        +- Join Inner
                                                                           :- SubqueryAlias `o`
                                                                           :  +- SubqueryAlias `originaldb`
                                                                           :     +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#32L]
                                                                           :        +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
                                                                           +- SubqueryAlias `n`
                                                                              +- SubqueryAlias `ncoldf`
                                                                                 +- Project [lineId#68L, CASE WHEN (lineId#68L = lineId#65L) THEN cast(null as string) ELSE uniformity_of_cell_size#12 END AS ncolumn#64]
                                                                                    +- Join LeftOuter, (lineId#68L = lineId#65L)
                                                                                       :- SubqueryAlias `o`
                                                                                       :  +- SubqueryAlias `originaldb`
                                                                                       :     +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#68L]
                                                                                       :        +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv
                                                                                       +- SubqueryAlias `n`
                                                                                          +- SubqueryAlias `niddf`
                                                                                             +- GlobalLimit 14
                                                                                                +- LocalLimit 14
                                                                                                   +- Project [lineId#65L]
                                                                                                      +- Sort [_nondeterministic#62 ASC NULLS FIRST], true
                                                                                                         +- Project [lineId#65L, rand(7866959123198424549) AS _nondeterministic#62]
                                                                                                            +- Project [lineId#65L]
                                                                                                               +- SubqueryAlias `originaldb`
                                                                                                                  +- Project [code_number#10, clump_thickness#11, uniformity_of_cell_size#12, uniformity_of_cell_shape#13, marginal_adhesion#14, single_epithelial_cell_size#15, bare_nuclei#16, bland_chromatin#17, normal_nucleoli#18, mitoses#19, class#20, monotonically_increasing_id() AS lineId#65L]
                                                                                                                     +- Relation[code_number#10,clump_thickness#11,uniformity_of_cell_size#12,uniformity_of_cell_shape#13,marginal_adhesion#14,single_epithelial_cell_size#15,bare_nuclei#16,bland_chromatin#17,normal_nucleoli#18,mitoses#19,class#20] csv

2019-12-06 12:28:11 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:28:11
2019-12-06 12:29:25 ERROR appraisal:54 â java.lang.IllegalArgumentException: Field "features" does not exist.
Available fields: clump_thickness, uniformity_of_cell_size, uniformity_of_cell_shape, marginal_adhesion, single_epithelial_cell_size, bare_nuclei, bland_chromatin, normal_nucleoli, mitoses, lineId
2019-12-06 12:30:09 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:30:09
2019-12-06 12:30:47 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 06/12/2019 12:30:47
2019-12-06 12:31:24 ERROR Executor:91 â Exception in task 6.0 in stage 67.0 (TID 2656)
org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$15$$anon$2.hasNext(WholeStageCodegenExec.scala:655)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 75.0 on feature index 8. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10$$anonfun$apply$4.apply(VectorIndexer.scala:370)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10$$anonfun$apply$4.apply(VectorIndexer.scala:363)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:363)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)
	... 21 more
2019-12-06 12:31:24 ERROR Executor:91 â Exception in task 0.0 in stage 67.0 (TID 2655)
org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$15$$anon$2.hasNext(WholeStageCodegenExec.scala:655)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 12.0 on feature index 8. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10$$anonfun$apply$4.apply(VectorIndexer.scala:370)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10$$anonfun$apply$4.apply(VectorIndexer.scala:363)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:363)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)
	... 21 more
2019-12-06 12:31:24 ERROR TaskSetManager:70 â Task 6 in stage 67.0 failed 1 times; aborting job
2019-12-06 12:31:24 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 67.0 failed 1 times, most recent failure: Lost task 6.0 in stage 67.0 (TID 2656, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$15$$anon$2.hasNext(WholeStageCodegenExec.scala:655)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 75.0 on feature index 8. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10$$anonfun$apply$4.apply(VectorIndexer.scala:370)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10$$anonfun$apply$4.apply(VectorIndexer.scala:363)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:363)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)
	at org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)
	... 21 more

Driver stacktrace:
