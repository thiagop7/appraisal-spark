2019-11-30 12:58:29 ERROR ImputationPlanRandomForestExec$:57 â Appraisal Spark - Wall start time: 30/11/2019 12:58:27
2019-11-30 12:58:29 ERROR ImputationPlanRandomForestExec$:58 â Parallel execution: true
2019-11-30 12:58:36 ERROR ImputationPlanRandomForestExec$:103 â Data count: 146
2019-11-30 12:59:44 ERROR ImputationPlanRandomForestExec$:57 â Appraisal Spark - Wall start time: 30/11/2019 12:59:41
2019-11-30 12:59:44 ERROR ImputationPlanRandomForestExec$:58 â Parallel execution: true
2019-11-30 12:59:44 ERROR ImputationPlanRandomForestExec$:57 â Appraisal Spark - Wall start time: 30/11/2019 12:59:41
2019-11-30 12:59:44 ERROR ImputationPlanRandomForestExec$:58 â Parallel execution: true
2019-11-30 12:59:51 ERROR ImputationPlanRandomForestExec$:103 â Data count: 146
2019-11-30 12:59:51 ERROR ImputationPlanRandomForestExec$:103 â Data count: 146
2019-11-30 13:01:00 ERROR ImputationPlanRandomForestExec$:57 â Appraisal Spark - Wall start time: 30/11/2019 13:00:58
2019-11-30 13:01:00 ERROR ImputationPlanRandomForestExec$:58 â Parallel execution: true
2019-11-30 13:01:00 ERROR ImputationPlanRandomForestExec$:57 â Appraisal Spark - Wall start time: 30/11/2019 13:00:58
2019-11-30 13:01:00 ERROR ImputationPlanRandomForestExec$:58 â Parallel execution: true
2019-11-30 13:01:07 ERROR ImputationPlanRandomForestExec$:103 â Data count: 146
2019-11-30 13:01:08 ERROR ImputationPlanRandomForestExec$:103 â Data count: 146
2019-11-30 13:03:54 ERROR ImputationPlan:312 â -------------------------------------
2019-11-30 13:03:54 ERROR ImputationPlan:312 â Running imputation plan: Imputation[RandomForest]
2019-11-30 13:03:54 ERROR ImputationPlan:312 â Missing rate at 10.0% in feature uniformity_of_cell_size
2019-11-30 13:03:54 ERROR ImputationPlan:312 â Batch for imputation before imputation strategy: 1
2019-11-30 13:03:54 ERROR ImputationPlan:312 â Running Imputation[RandomForest] | params: Map(learningRate -> 0.1, k -> 2, imputationFeature -> uniformity_of_cell_size, kLimit -> 146, varianceComplete -> 8.031713267029476, calcFeatures -> [Ljava.lang.String;@3faae868, T -> 3)
2019-11-30 13:03:54 ERROR ImputationPlan:313 â Error executing imputation plan: Imputation[RandomForest]
java.lang.NullPointerException
	at org.apache.spark.ml.feature.VectorIndexer.fit(VectorIndexer.scala:144)
	at appraisal.spark.algorithm.RandomForest.run(RandomForest.scala:62)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$7.apply(ImputationPlan.scala:243)
	at appraisal.spark.engine.ImputationPlan$$anonfun$7.apply(ImputationPlan.scala:241)
	at scala.collection.parallel.AugmentedIterableIterator$class.map2combiner(RemainsIterator.scala:115)
	at scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:62)
	at scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1054)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
	at scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1051)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)
	at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)
	at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)
	at scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)
	at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)
	at scala.collection.parallel.ExecutionContextTasks$class.executeAndWaitResult(Tasks.scala:558)
	at scala.collection.parallel.ExecutionContextTaskSupport.executeAndWaitResult(TaskSupport.scala:80)
	at scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
	at scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)
	at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)
	at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)
	at scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)
	at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)
	at scala.collection.parallel.ExecutionContextTasks$class.executeAndWaitResult(Tasks.scala:558)
	at scala.collection.parallel.ExecutionContextTaskSupport.executeAndWaitResult(TaskSupport.scala:80)
	at scala.collection.parallel.ParIterableLike$class.map(ParIterableLike.scala:499)
	at scala.collection.parallel.immutable.ParVector.map(ParVector.scala:38)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:241)
	at appraisal.spark.algorithm.Boost$$anonfun$run$2.apply(Boost.scala:91)
	at appraisal.spark.algorithm.Boost$$anonfun$run$2.apply(Boost.scala:89)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)
	at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)
	at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)
	at scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)
	at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)
	at scala.collection.parallel.ExecutionContextTasks$class.executeAndWaitResult(Tasks.scala:558)
	at scala.collection.parallel.ExecutionContextTaskSupport.executeAndWaitResult(TaskSupport.scala:80)
	at scala.collection.parallel.ParIterableLike$class.foreach(ParIterableLike.scala:463)
	at scala.collection.parallel.immutable.ParVector.foreach(ParVector.scala:38)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:89)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:337)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$1.apply(Crowner.scala:71)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$1.apply(Crowner.scala:69)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2019-11-30 13:03:54 ERROR ImputationPlan:331 â -------------------------------------
2019-11-30 13:03:54 ERROR ImputationPlan:331 â Running imputation plan: Imputation[RandomForest]
2019-11-30 13:03:54 ERROR ImputationPlan:331 â Missing rate at 10.0% in feature uniformity_of_cell_size
2019-11-30 13:03:54 ERROR ImputationPlan:331 â Batch for imputation before imputation strategy: 1
2019-11-30 13:03:54 ERROR ImputationPlan:331 â Running Imputation[RandomForest] | params: Map(learningRate -> 0.1, k -> 2, imputationFeature -> uniformity_of_cell_size, kLimit -> 146, varianceComplete -> 8.031713267029476, calcFeatures -> [Ljava.lang.String;@3faae868, T -> 3)
2019-11-30 13:03:54 ERROR ImputationPlan:331 â Total plan execution time: 66 seconds, 1 minutes, 0 hours.
2019-11-30 13:03:54 ERROR Crowner:83 â Executed plans: 1 / 9 : 12%.
2019-11-30 13:15:07 ERROR RandomForestExec$:19 â Appraisal Spark - Wall start time: 30/11/2019 13:15:07
2019-11-30 13:15:10 ERROR appraisal:21 â org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/oem/Workspace/mestrado/appraisalAdaboost-spark/data/databreast-cancer-wisconsin.reduced.csv;
2019-11-30 13:18:16 ERROR RandomForestExec$:19 â Appraisal Spark - Wall start time: 30/11/2019 13:18:16
2019-11-30 13:18:16 ERROR RandomForestExec$:19 â Appraisal Spark - Wall start time: 30/11/2019 13:18:16
2019-11-30 13:18:57 ERROR appraisal:21 â java.util.NoSuchElementException: key not found: T
2019-11-30 13:19:28 ERROR RandomForestExec$:19 â Appraisal Spark - Wall start time: 30/11/2019 13:19:28
2019-11-30 13:19:39 ERROR appraisal:21 â java.util.NoSuchElementException: key not found: T
2019-11-30 13:20:15 ERROR RandomForestExec$:19 â Appraisal Spark - Wall start time: 30/11/2019 13:20:15
2019-11-30 13:20:26 ERROR appraisal:21 â java.util.NoSuchElementException: key not found: learningRate
2019-11-30 13:22:39 ERROR RandomForestExec$:19 â Appraisal Spark - Wall start time: 30/11/2019 13:22:39
2019-11-30 13:24:50 ERROR appraisal:21 â java.lang.IllegalArgumentException: Field "features" does not exist.
Available fields: code_number, clump_thickness, uniformity_of_cell_size, uniformity_of_cell_shape, marginal_adhesion, single_epithelial_cell_size, bare_nuclei, bland_chromatin, normal_nucleoli, mitoses, class, lineId, originalValue
2019-11-30 13:38:43 ERROR RandomForestExec$:19 â Appraisal Spark - Wall start time: 30/11/2019 13:38:43
2019-11-30 13:38:43 ERROR RandomForestExec$:19 â Appraisal Spark - Wall start time: 30/11/2019 13:38:43
2019-11-30 13:39:26 ERROR appraisal:21 â java.lang.IllegalArgumentException: Field "features" does not exist.
Available fields: code_number, clump_thickness, uniformity_of_cell_size, uniformity_of_cell_shape, marginal_adhesion, single_epithelial_cell_size, bare_nuclei, bland_chromatin, normal_nucleoli, mitoses, class, lineId, originalValue
2019-11-30 13:48:13 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 13:48:13
2019-11-30 14:29:35 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 14:29:35
2019-11-30 14:36:02 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 14:36:02
2019-11-30 14:36:02 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 14:36:02
2019-11-30 14:50:15 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 14:50:15
2019-11-30 16:08:45 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:08:45
2019-11-30 16:30:45 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:30:45
2019-11-30 16:31:09 ERROR appraisal:54 â java.lang.ClassNotFoundException: scala.Any
2019-11-30 16:31:50 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:31:50
2019-11-30 16:32:02 ERROR appraisal:54 â java.lang.ClassNotFoundException: scala.Any
2019-11-30 16:32:20 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:32:20
2019-11-30 16:32:29 ERROR appraisal:54 â java.lang.ClassNotFoundException: scala.Any
2019-11-30 16:32:59 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:32:59
2019-11-30 16:33:08 ERROR appraisal:54 â java.lang.ClassNotFoundException: scala.Any
2019-11-30 16:35:09 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:35:09
2019-11-30 16:36:15 ERROR appraisal:54 â java.lang.ClassNotFoundException: scala.Any
2019-11-30 16:41:40 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:41:40
2019-11-30 16:44:25 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:44:25
2019-11-30 16:47:14 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:47:14
2019-11-30 16:47:14 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:47:14
2019-11-30 16:47:28 ERROR appraisal:54 â java.lang.ClassNotFoundException: scala.Any
2019-11-30 16:47:28 ERROR appraisal:54 â java.lang.ClassNotFoundException: scala.Any
2019-11-30 16:51:20 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 16:51:20
2019-11-30 17:09:51 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 17:09:51
2019-11-30 17:15:44 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 17:15:44
2019-11-30 17:23:57 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 17:23:57
2019-11-30 18:04:22 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 18:04:22
2019-11-30 18:04:31 ERROR Executor:91 â Exception in task 0.0 in stage 17.0 (TID 17)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of array<double>
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayData, ArrayType(DoubleType,true), toArrayData, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, features), ArrayType(DoubleType,true)), true, false) AS features#433
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, indexedFeatures), DoubleType) AS indexedFeatures#434
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:292)
	at org.apache.spark.sql.SparkSession$$anonfun$4.apply(SparkSession.scala:593)
	at org.apache.spark.sql.SparkSession$$anonfun$4.apply(SparkSession.scala:593)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of array<double>
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:289)
	... 25 more
2019-11-30 18:04:31 ERROR TaskSetManager:70 â Task 0 in stage 17.0 failed 1 times; aborting job
2019-11-30 18:04:31 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 17, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of array<double>
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayData, ArrayType(DoubleType,true), toArrayData, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, features), ArrayType(DoubleType,true)), true, false) AS features#433
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, indexedFeatures), DoubleType) AS indexedFeatures#434
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:292)
	at org.apache.spark.sql.SparkSession$$anonfun$4.apply(SparkSession.scala:593)
	at org.apache.spark.sql.SparkSession$$anonfun$4.apply(SparkSession.scala:593)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of array<double>
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:289)
	... 25 more

Driver stacktrace:
2019-11-30 18:04:54 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 18:04:54
2019-11-30 18:06:26 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 18:06:26
2019-11-30 18:06:34 ERROR Executor:91 â Exception in task 0.0 in stage 17.0 (TID 17)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of array<double>
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayData, ArrayType(DoubleType,true), toArrayData, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, features), ArrayType(DoubleType,true)), true, false) AS features#433
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:292)
	at org.apache.spark.sql.SparkSession$$anonfun$4.apply(SparkSession.scala:593)
	at org.apache.spark.sql.SparkSession$$anonfun$4.apply(SparkSession.scala:593)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of array<double>
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:289)
	... 25 more
2019-11-30 18:06:35 ERROR TaskSetManager:70 â Task 0 in stage 17.0 failed 1 times; aborting job
2019-11-30 18:06:35 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 17, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of array<double>
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayData, ArrayType(DoubleType,true), toArrayData, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, features), ArrayType(DoubleType,true)), true, false) AS features#433
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:292)
	at org.apache.spark.sql.SparkSession$$anonfun$4.apply(SparkSession.scala:593)
	at org.apache.spark.sql.SparkSession$$anonfun$4.apply(SparkSession.scala:593)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of array<double>
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:289)
	... 25 more

Driver stacktrace:
2019-11-30 18:11:12 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 18:11:12
2019-11-30 18:11:20 ERROR Executor:91 â Exception in task 0.0 in stage 16.0 (TID 16)
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be cast to [D
	at appraisal.spark.algorithm.RandomForest$$anonfun$6.apply(RandomForest.scala:58)
	at appraisal.spark.algorithm.RandomForest$$anonfun$6.apply(RandomForest.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-11-30 18:11:20 ERROR TaskSetManager:70 â Task 0 in stage 16.0 failed 1 times; aborting job
2019-11-30 18:11:20 ERROR appraisal:54 â org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16, localhost, executor driver): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be cast to [D
	at appraisal.spark.algorithm.RandomForest$$anonfun$6.apply(RandomForest.scala:58)
	at appraisal.spark.algorithm.RandomForest$$anonfun$6.apply(RandomForest.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-11-30 18:46:48 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 18:46:48
2019-11-30 18:49:40 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 18:49:40
2019-11-30 19:15:54 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 19:15:54
2019-11-30 19:17:04 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 19:17:04
2019-11-30 19:17:57 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 19:17:57
2019-11-30 19:18:31 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 19:18:31
2019-11-30 22:33:20 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:33:20
2019-11-30 22:33:20 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:33:20
2019-11-30 22:33:23 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:33:23
2019-11-30 22:35:34 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:35:34
2019-11-30 22:35:34 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:35:34
2019-11-30 22:38:16 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:38:16
2019-11-30 22:38:16 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:38:16
2019-11-30 22:39:16 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:39:16
2019-11-30 22:49:17 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:49:17
2019-11-30 22:49:27 ERROR appraisal:54 â org.apache.spark.sql.AnalysisException: Resolved attribute(s) uniformity_of_cell_size#236 missing from features#437 in operator !Project [features#437, UDF(cast(uniformity_of_cell_size#236 as string)) AS uniformity_of_cell_size#439].;;
!Project [features#437, UDF(cast(uniformity_of_cell_size#236 as string)) AS uniformity_of_cell_size#439]
+- Project [value#435 AS features#437]
   +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true, false) AS value#435]
      +- ExternalRDD [obj#434]

2019-11-30 22:50:55 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:50:55
2019-11-30 22:50:56 ERROR RandomForestExec$:52 â Appraisal Spark - Wall start time: 30/11/2019 22:50:56
