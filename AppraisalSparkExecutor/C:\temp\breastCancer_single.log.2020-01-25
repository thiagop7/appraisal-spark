2020-01-25 14:06:52 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 14:06:50
2020-01-25 14:06:52 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 14:06:59 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 14:08:32 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 14:08:32 ERROR ImputationPlan:388 â Running imputation plan: Imputation[AdaboostR2]
2020-01-25 14:08:32 ERROR ImputationPlan:388 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 14:08:32 ERROR ImputationPlan:388 â Batch for imputation before imputation strategy: 1
2020-01-25 14:08:32 ERROR ImputationPlan:388 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@1766b7, calcFeatures -> [Ljava.lang.String;@56f3f9da)
2020-01-25 14:08:32 ERROR ImputationPlan:388 â ImputationResult: 3.0,1.0,3.0,1.0,5.0,1.0,1.0,2.0,4.0,3.0,1.0,8.0,2.0,1.0,6.0,3.0,1.0,5.0,6.0,1.0,1.0,3.0,10.0,4.0,5.0,10.0,6.0,1.0,10.0,1.0,1.0,8.0,1.0,1.0,1.0,1.0,7.0,7.0,5.0,4.0,5.0,10.0,3.0,3.0,7.0,2.0,2.0,5.0,4.0,5.0,1.0,6.0,10.0,5.0,4.0,5.0,10.0,4.0,5.0,5.0,1.0,1.0,1.0,3.0,4.0,5.0,5.0,8.0,3.0,3.0,4.0,1.0,4.0,3.0,3.0,2.0,2.0,8.0,6.0,4.0,5.0,8.0,1.0,1.0,3.0,6.0,5.0,1.0,9.0,2.0,3.0,8.0,1.0,2.0,8.0,1.0,1.0,1.0,3.0,7.0,4.0,3.0,7.0,1.0,10.0,10.0,2.0,1.0,1.0,3.0,4.0,3.0,5.0,8.0,10.0,1.0,10.0,9.0,5.0,6.0,4.0,2.0,1.0,5.0,3.0,2.0,9.0,10.0,10.0,2.0,2.0,6.0,10.0,2.0,2.0,7.0,2.0,5.0,9.0,3.0,1.0,5.0,1.0,1.0,2.0,6.0
2020-01-25 14:08:32 ERROR ImputationPlan:388 â OriginalValues: 5.0,5.0,3.0,3.0,9.0,4.0,2.0,4.0,10.0,10.0,1.0,8.0,3.0,5.0,10.0,5.0,4.0,9.0,5.0,1.0,3.0,8.0,8.0,5.0,4.0,10.0,3.0,1.0,10.0,3.0,1.0,6.0,1.0,1.0,2.0,3.0,10.0,8.0,10.0,7.0,7.0,10.0,5.0,3.0,7.0,2.0,2.0,5.0,4.0,5.0,1.0,6.0,10.0,5.0,4.0,5.0,10.0,4.0,5.0,5.0,1.0,1.0,1.0,3.0,4.0,5.0,5.0,8.0,3.0,3.0,4.0,1.0,4.0,3.0,3.0,2.0,2.0,8.0,6.0,4.0,5.0,8.0,1.0,1.0,3.0,6.0,5.0,1.0,9.0,2.0,3.0,8.0,1.0,2.0,8.0,1.0,1.0,1.0,3.0,7.0,4.0,3.0,7.0,1.0,10.0,10.0,2.0,1.0,1.0,3.0,4.0,3.0,5.0,8.0,10.0,1.0,10.0,9.0,5.0,6.0,4.0,2.0,1.0,5.0,3.0,2.0,9.0,10.0,10.0,2.0,2.0,6.0,10.0,2.0,2.0,7.0,2.0,5.0,9.0,3.0,1.0,5.0,1.0,1.0,2.0,6.0
2020-01-25 14:08:32 ERROR ImputationPlan:388 â totalError: 1.4594988819116546
2020-01-25 14:08:32 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 14:08:32 ERROR ImputationPlan:388 â avgPercentError: 1.433819186739921
2020-01-25 14:08:32 ERROR ImputationPlan:388 â varianceCompleteError: 8.8339744792644
2020-01-25 14:08:32 ERROR ImputationPlan:388 â varianceImputedError: 8.248802438670333
2020-01-25 14:08:32 ERROR ImputationPlan:388 â weights: 1.0
2020-01-25 14:08:32 ERROR ImputationPlan:388 â varianceImputedErrorTotal: 8.315443798085946
2020-01-25 14:08:32 ERROR ImputationPlan:388 â varianceCompleteErrorTotal: 8.833974479264407
2020-01-25 14:08:32 ERROR ImputationPlan:388 â RMSE: 1.4594988819116546
2020-01-25 14:08:32 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 14:08:32 ERROR ImputationPlan:388 â Total plan execution time: 88 seconds, 1 minutes, 0 hours.
2020-01-25 14:08:32 ERROR ImputationPlanAdaboostR2Exec$:241 â java.lang.NullPointerException
2020-01-25 14:13:34 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 14:13:32
2020-01-25 14:13:34 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 14:13:42 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 14:15:39 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 14:15:39 ERROR ImputationPlan:388 â Running imputation plan: Imputation[AdaboostR2]
2020-01-25 14:15:39 ERROR ImputationPlan:388 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 14:15:39 ERROR ImputationPlan:388 â Batch for imputation before imputation strategy: 1
2020-01-25 14:15:39 ERROR ImputationPlan:388 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@b38dc7d, calcFeatures -> [Ljava.lang.String;@746cb25d)
2020-01-25 14:15:39 ERROR ImputationPlan:388 â ImputationResult: 1.0,1.0,10.0,1.0,1.0,5.0,1.0,1.0,1.0,8.0,5.0,1.0,6.0,5.0,4.0,1.0,1.0,10.0,1.0,6.0,1.0,3.0,10.0,2.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,4.0,10.0,1.0,1.0,3.0,1.0,6.0,1.0,1.0,8.0,2.0,2.0,5.0,9.0,3.0,4.0,5.0,3.0,2.0,10.0,5.0,9.0,5.0,3.0,4.0,9.0,10.0,1.0,8.0,1.0,5.0,4.0,10.0,4.0,2.0,7.0,8.0,3.0,1.0,10.0,2.0,1.0,3.0,2.0,3.0,1.0,5.0,6.0,9.0,1.0,2.0,3.0,2.0,8.0,5.0,5.0,6.0,10.0,3.0,1.0,2.0,1.0,10.0,1.0,4.0,4.0,3.0,1.0,5.0,1.0,7.0,1.0,10.0,8.0,6.0,7.0,10.0,3.0,2.0,3.0,10.0,5.0,1.0,3.0,2.0,4.0,10.0,10.0,2.0,3.0,4.0,10.0,3.0,9.0,3.0,1.0,10.0,2.0,10.0,5.0,2.0,8.0,2.0,3.0,3.0,3.0,5.0,1.0,5.0,5.0,4.0,1.0,2.0,6.0
2020-01-25 14:15:39 ERROR ImputationPlan:388 â OriginalValues: 5.0,6.0,8.0,5.0,2.0,9.0,2.0,1.0,1.0,8.0,7.0,5.0,10.0,6.0,10.0,1.0,1.0,8.0,1.0,1.0,1.0,6.0,5.0,5.0,8.0,4.0,3.0,4.0,1.0,5.0,3.0,3.0,5.0,10.0,1.0,4.0,7.0,4.0,8.0,4.0,1.0,7.0,5.0,2.0,5.0,9.0,3.0,4.0,5.0,3.0,2.0,10.0,5.0,9.0,5.0,3.0,4.0,9.0,10.0,1.0,8.0,1.0,5.0,4.0,10.0,4.0,2.0,7.0,8.0,3.0,1.0,10.0,2.0,1.0,3.0,2.0,3.0,1.0,5.0,6.0,9.0,1.0,2.0,3.0,2.0,8.0,5.0,5.0,6.0,10.0,3.0,1.0,2.0,1.0,10.0,1.0,4.0,4.0,3.0,1.0,5.0,1.0,7.0,1.0,10.0,8.0,6.0,7.0,10.0,3.0,2.0,3.0,10.0,5.0,1.0,3.0,2.0,4.0,10.0,10.0,2.0,3.0,4.0,10.0,3.0,9.0,3.0,1.0,10.0,2.0,10.0,5.0,2.0,8.0,2.0,3.0,3.0,3.0,5.0,1.0,5.0,5.0,4.0,1.0,2.0,6.0
2020-01-25 14:15:39 ERROR ImputationPlan:388 â totalError: 1.5282723704746364
2020-01-25 14:15:39 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 14:15:39 ERROR ImputationPlan:388 â avgPercentError: 1.5136930042245667
2020-01-25 14:15:39 ERROR ImputationPlan:388 â varianceCompleteError: 8.8339744792644
2020-01-25 14:15:39 ERROR ImputationPlan:388 â varianceImputedError: 9.445815390074335
2020-01-25 14:15:39 ERROR ImputationPlan:388 â weights: 1.0
2020-01-25 14:15:39 ERROR ImputationPlan:388 â varianceImputedErrorTotal: 9.538562582097951
2020-01-25 14:15:39 ERROR ImputationPlan:388 â varianceCompleteErrorTotal: 8.833974479264405
2020-01-25 14:15:39 ERROR ImputationPlan:388 â RMSE: 1.5282723704746364
2020-01-25 14:15:39 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 14:15:39 ERROR ImputationPlan:388 â Total plan execution time: 113 seconds, 1 minutes, 0 hours.
2020-01-25 14:15:57 ERROR ImputationPlanAdaboostR2Exec$:241 â java.lang.NullPointerException
2020-01-25 15:21:57 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 15:21:55
2020-01-25 15:21:57 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 15:22:02 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 15:24:11 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 15:24:11 ERROR ImputationPlan:388 â Running imputation plan: Imputation[AdaboostR2]
2020-01-25 15:24:11 ERROR ImputationPlan:388 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 15:24:11 ERROR ImputationPlan:388 â Batch for imputation before imputation strategy: 1
2020-01-25 15:24:11 ERROR ImputationPlan:388 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@2f0b7b6d, calcFeatures -> [Ljava.lang.String;@1766b7)
2020-01-25 15:24:11 ERROR ImputationPlan:388 â ImputationResult: 3.0,1.0,10.0,1.0,3.0,1.0,3.0,7.0,1.0,1.0,3.0,5.0,4.0,1.0,1.0,5.0,1.0,10.0,6.0,1.0,1.0,1.0,1.0,8.0,2.0,1.0,1.0,3.0,1.0,10.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,3.0,1.0,1.0,10.0,1.0,2.0,2.0,6.0,5.0,1.0,5.0,5.0,2.0,2.0,10.0,10.0,6.0,2.0,10.0,10.0,10.0,3.0,2.0,7.0,4.0,2.0,3.0,5.0,2.0,3.0,9.0,4.0,5.0,1.0,3.0,3.0,1.0,9.0,4.0,10.0,1.0,5.0,10.0,5.0,4.0,1.0,5.0,3.0,2.0,5.0,1.0,1.0,1.0,3.0,8.0,2.0,4.0,1.0,3.0,10.0,1.0,6.0,10.0,4.0,5.0,5.0,9.0,1.0,1.0,3.0,2.0,7.0,8.0,8.0,8.0,8.0,3.0,2.0,1.0,3.0,1.0,7.0,5.0,3.0,10.0,8.0,4.0,10.0,5.0,5.0,10.0,7.0,7.0,1.0,5.0,3.0,2.0,1.0,8.0,2.0,5.0,9.0,7.0,3.0,3.0,8.0,9.0,8.0,3.0
2020-01-25 15:24:11 ERROR ImputationPlan:388 â OriginalValues: 5.0,6.0,5.0,2.0,10.0,2.0,3.0,10.0,1.0,1.0,5.0,6.0,10.0,4.0,4.0,9.0,2.0,10.0,3.0,1.0,4.0,1.0,1.0,6.0,8.0,2.0,1.0,5.0,4.0,10.0,1.0,3.0,3.0,5.0,1.0,4.0,3.0,1.0,4.0,1.0,6.0,4.0,5.0,2.0,6.0,5.0,1.0,5.0,5.0,2.0,2.0,10.0,10.0,6.0,2.0,10.0,10.0,10.0,3.0,2.0,7.0,4.0,2.0,3.0,5.0,2.0,3.0,9.0,4.0,5.0,1.0,3.0,3.0,1.0,9.0,4.0,10.0,1.0,5.0,10.0,5.0,4.0,1.0,5.0,3.0,2.0,5.0,1.0,1.0,1.0,3.0,8.0,2.0,4.0,1.0,3.0,10.0,1.0,6.0,10.0,4.0,5.0,5.0,9.0,1.0,1.0,3.0,2.0,7.0,8.0,8.0,8.0,8.0,3.0,2.0,1.0,3.0,1.0,7.0,5.0,3.0,10.0,8.0,4.0,10.0,5.0,5.0,10.0,7.0,7.0,1.0,5.0,3.0,2.0,1.0,8.0,2.0,5.0,9.0,7.0,3.0,3.0,8.0,9.0,8.0,3.0
2020-01-25 15:24:11 ERROR ImputationPlan:388 â totalError: 1.4988580127769788
2020-01-25 15:24:11 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 15:24:11 ERROR ImputationPlan:388 â avgPercentError: 1.4554027246787535
2020-01-25 15:24:11 ERROR ImputationPlan:388 â varianceCompleteError: 8.8339744792644
2020-01-25 15:24:11 ERROR ImputationPlan:388 â varianceImputedError: 9.366139255818432
2020-01-25 15:24:11 ERROR ImputationPlan:388 â weights: 1.0
2020-01-25 15:24:11 ERROR ImputationPlan:388 â varianceImputedErrorTotal: 9.49057046350159
2020-01-25 15:24:11 ERROR ImputationPlan:388 â varianceCompleteErrorTotal: 8.833974479264404
2020-01-25 15:24:11 ERROR ImputationPlan:388 â RMSE: 1.4988580127769788
2020-01-25 15:24:11 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 15:24:11 ERROR ImputationPlan:388 â Total plan execution time: 127 seconds, 2 minutes, 0 hours.
2020-01-25 15:25:57 ERROR Crowner:103 â Executed plans: 1 / 9 : 12%.
2020-01-25 15:39:41 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 15:39:39
2020-01-25 15:39:41 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 15:39:47 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 15:42:45 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 15:42:45 ERROR ImputationPlan:388 â Running imputation plan: Imputation[AdaboostR2]
2020-01-25 15:42:45 ERROR ImputationPlan:388 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 15:42:45 ERROR ImputationPlan:388 â Batch for imputation before imputation strategy: 1
2020-01-25 15:42:45 ERROR ImputationPlan:388 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@4528a8bf, calcFeatures -> [Ljava.lang.String;@1927c2c6)
2020-01-25 15:42:45 ERROR ImputationPlan:388 â ImputationResult: 1.0,1.0,3.0,1.0,3.0,3.0,1.0,1.0,3.0,3.0,8.0,6.0,4.0,5.0,1.0,1.0,1.0,10.0,6.0,5.0,1.0,6.0,1.0,8.0,1.0,1.0,1.0,10.0,1.0,10.0,2.0,1.0,4.0,4.0,5.0,1.0,10.0,1.0,2.0,3.0,10.0,1.0,8.0,6.0,6.0,2.0,3.0,5.0,5.0,2.0,2.0,8.0,4.0,9.0,3.0,5.0,3.0,1.0,1.0,1.0,8.0,3.0,3.0,3.0,8.0,2.0,7.0,7.0,1.0,4.0,1.0,5.0,2.0,1.0,3.0,5.0,3.0,3.0,9.0,10.0,1.0,7.0,1.0,5.0,4.0,2.0,3.0,4.0,6.0,9.0,9.0,6.0,4.0,4.0,10.0,3.0,1.0,1.0,3.0,1.0,4.0,1.0,5.0,1.0,5.0,1.0,3.0,5.0,1.0,1.0,8.0,2.0,8.0,10.0,6.0,10.0,5.0,2.0,10.0,1.0,2.0,6.0,8.0,8.0,4.0,3.0,5.0,5.0,10.0,8.0,10.0,8.0,2.0,4.0,2.0,5.0,3.0,5.0,5.0,10.0,1.0,5.0,4.0,5.0,2.0,2.0
2020-01-25 15:42:45 ERROR ImputationPlan:388 â OriginalValues: 5.0,3.0,5.0,2.0,10.0,10.0,4.0,3.0,3.0,10.0,8.0,10.0,10.0,5.0,4.0,4.0,1.0,10.0,3.0,9.0,1.0,1.0,1.0,6.0,2.0,1.0,1.0,10.0,3.0,10.0,2.0,1.0,5.0,7.0,2.0,5.0,10.0,1.0,3.0,7.0,9.0,4.0,7.0,6.0,6.0,2.0,3.0,5.0,5.0,2.0,2.0,8.0,4.0,9.0,3.0,5.0,3.0,1.0,1.0,1.0,8.0,3.0,3.0,3.0,8.0,2.0,7.0,7.0,1.0,4.0,1.0,5.0,2.0,1.0,3.0,5.0,3.0,3.0,9.0,10.0,1.0,7.0,1.0,5.0,4.0,2.0,3.0,4.0,6.0,9.0,9.0,6.0,4.0,4.0,10.0,3.0,1.0,1.0,3.0,1.0,4.0,1.0,5.0,1.0,5.0,1.0,3.0,5.0,1.0,1.0,8.0,2.0,8.0,10.0,6.0,10.0,5.0,2.0,10.0,1.0,2.0,6.0,8.0,8.0,4.0,3.0,5.0,5.0,10.0,8.0,10.0,8.0,2.0,4.0,2.0,5.0,3.0,5.0,5.0,10.0,1.0,5.0,4.0,5.0,2.0,2.0
2020-01-25 15:42:45 ERROR ImputationPlan:388 â RMSE: 1.6069199671489298
2020-01-25 15:42:45 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 15:42:45 ERROR ImputationPlan:388 â avgPercentError: 1.5698226849563042
2020-01-25 15:42:45 ERROR ImputationPlan:388 â varianceCompleteError: 8.8339744792644
2020-01-25 15:42:45 ERROR ImputationPlan:388 â varianceImputedError: 8.235544587990525
2020-01-25 15:42:45 ERROR ImputationPlan:388 â weights: 1.0
2020-01-25 15:42:45 ERROR ImputationPlan:388 â varianceImputedErrorTotal: 8.493713642334392
2020-01-25 15:42:45 ERROR ImputationPlan:388 â varianceCompleteErrorTotal: 8.833974479264409
2020-01-25 15:42:45 ERROR ImputationPlan:388 â RMSE: 1.6069199671489298
2020-01-25 15:42:45 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 15:42:45 ERROR ImputationPlan:388 â Total plan execution time: 176 seconds, 2 minutes, 0 hours.
2020-01-25 15:42:45 ERROR Crowner:103 â Executed plans: 1 / 9 : 12%.
2020-01-25 15:44:45 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 15:44:45 ERROR ImputationPlan:388 â Running imputation plan: Imputation[AdaboostR2]
2020-01-25 15:44:45 ERROR ImputationPlan:388 â Missing rate at 30.0% in feature uniformity_of_cell_size
2020-01-25 15:44:45 ERROR ImputationPlan:388 â Batch for imputation before imputation strategy: 1
2020-01-25 15:44:45 ERROR ImputationPlan:388 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> uniformity_of_cell_size, kLimit -> 10, varianceComplete -> 8.031713267029476, features -> [Ljava.lang.String;@4528a8bf, calcFeatures -> [Ljava.lang.String;@5486bda)
2020-01-25 15:44:45 ERROR ImputationPlan:388 â ImputationResult: 1.0,10.0,2.0,1.0,2.0,1.0,1.0,6.999999999999999,1.0,3.0,1.0,6.999999999999999,1.0,3.0,1.0,5.0,4.0,5.0,10.0,1.0,3.0,6.999999999999999,2.0,8.0,8.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,10.0,6.999999999999999,2.0,1.0,2.0,5.0,6.0,1.0,2.0,3.0,1.0,5.0,10.0,1.0,1.0,2.0,1.0,1.0,2.0,3.0,5.0,1.0,1.0,1.0,3.0,4.0,10.0,6.999999999999999,3.0,1.0,2.0,1.0,6.0,1.0,1.0,3.0,1.0,5.0,3.0,10.0,1.0,3.0,2.0,1.0,2.0,3.0,10.0,6.0,3.0,5.0,10.0,4.0,1.0,4.0,1.0,3.0,6.0,1.0,1.0,1.0,6.0,8.0,2.0,4.0,2.0,6.999999999999999,1.0,10.0,1.0,1.0,1.0,1.0,6.0,10.0,1.0,1.0,5.0,1.0,1.0,10.0,1.0,2.0,1.0,6.0,2.0,1.0,5.0,1.0,10.0,1.0,3.0,5.0,5.0,1.0,1.0,3.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0
2020-01-25 15:44:45 ERROR ImputationPlan:388 â OriginalValues: 1.0,10.0,3.0,1.0,1.0,1.0,1.0,7.0,2.0,4.0,1.0,8.0,1.0,1.0,1.0,5.0,3.0,5.0,10.0,1.0,1.0,5.0,1.0,6.0,7.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,10.0,3.0,1.0,1.0,10.0,7.0,5.0,1.0,3.0,3.0,1.0,5.0,10.0,1.0,1.0,2.0,1.0,1.0,2.0,3.0,5.0,1.0,1.0,1.0,3.0,4.0,10.0,8.0,3.0,1.0,2.0,1.0,6.0,1.0,1.0,3.0,1.0,5.0,3.0,10.0,1.0,3.0,2.0,1.0,2.0,3.0,10.0,6.0,3.0,5.0,10.0,4.0,1.0,4.0,1.0,3.0,6.0,1.0,1.0,1.0,6.0,8.0,2.0,4.0,2.0,7.0,1.0,10.0,1.0,1.0,1.0,1.0,6.0,10.0,1.0,1.0,5.0,1.0,1.0,10.0,1.0,2.0,1.0,6.0,2.0,1.0,5.0,1.0,10.0,1.0,3.0,5.0,5.0,1.0,1.0,3.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0
2020-01-25 15:44:45 ERROR ImputationPlan:388 â RMSE: 0.8797571271321001
2020-01-25 15:44:45 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 15:44:45 ERROR ImputationPlan:388 â avgPercentError: 0.7343429649016712
2020-01-25 15:44:45 ERROR ImputationPlan:388 â varianceCompleteError: 8.031713267029476
2020-01-25 15:44:45 ERROR ImputationPlan:388 â varianceImputedError: 7.762519959355482
2020-01-25 15:44:45 ERROR ImputationPlan:388 â weights: 4.624972813284271
2020-01-25 15:44:45 ERROR ImputationPlan:388 â varianceImputedErrorTotal: 7.797382248076573
2020-01-25 15:44:45 ERROR ImputationPlan:388 â varianceCompleteErrorTotal: 8.031713267029476
2020-01-25 15:44:45 ERROR ImputationPlan:388 â RMSE: 0.8797571271321001
2020-01-25 15:44:45 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 15:44:45 ERROR ImputationPlan:388 â Total plan execution time: 119 seconds, 1 minutes, 0 hours.
2020-01-25 15:44:45 ERROR Crowner:103 â Executed plans: 2 / 9 : 23%.
2020-01-25 15:50:40 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 15:50:40 ERROR ImputationPlan:388 â Running imputation plan: Imputation[AdaboostR2]
2020-01-25 15:50:40 ERROR ImputationPlan:388 â Missing rate at 30.0% in feature uniformity_of_cell_shape
2020-01-25 15:50:40 ERROR ImputationPlan:388 â Batch for imputation before imputation strategy: 1
2020-01-25 15:50:40 ERROR ImputationPlan:388 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> uniformity_of_cell_shape, kLimit -> 10, varianceComplete -> 7.857055732782882, features -> [Ljava.lang.String;@4528a8bf, calcFeatures -> [Ljava.lang.String;@65e1e974)
2020-01-25 15:50:40 ERROR ImputationPlan:388 â ImputationResult: 1.0,10.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,5.0,4.0,8.0,1.0,6.0,2.0,4.0,1.0,1.0,1.0,2.0,1.0,6.0,2.0,1.0,8.0,1.0,1.0,1.0,4.0,3.0,10.0,2.0,1.0,1.0,3.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,4.0,1.0,3.0,1.0,1.0,1.0,4.0,6.999999999999999,1.0,4.0,1.0,2.0,1.0,1.0,1.0,10.0,1.0,5.0,6.0,1.0,10.0,1.0,4.0,1.0,8.0,3.0,5.0,6.999999999999999,5.0,4.0,1.0,5.0,1.0,1.0,1.0,10.0,4.0,3.0,6.999999999999999,6.0,1.0,1.0,1.0,6.0,1.0,1.0,4.0,1.0,1.0,4.0,1.0,1.0,8.0,1.0,5.0,1.0,2.0,10.0,3.0,1.0,1.0,1.0,1.0,2.0,8.0,1.0,1.0,6.999999999999999,10.0,1.0,6.999999999999999,6.999999999999999,1.0,3.0,1.0,6.0,1.0,1.0,6.0,2.0,1.0,6.0,3.0,1.0,10.0,5.0,4.0,3.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
2020-01-25 15:50:40 ERROR ImputationPlan:388 â OriginalValues: 1.0,8.0,2.0,5.0,2.0,1.0,1.0,3.0,4.0,5.0,4.0,5.0,3.0,5.0,1.0,5.0,3.0,2.0,1.0,10.0,3.0,6.0,5.0,2.0,8.0,6.0,4.0,1.0,3.0,8.0,10.0,2.0,1.0,1.0,5.0,1.0,1.0,10.0,2.0,3.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,4.0,7.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,10.0,1.0,5.0,6.0,1.0,10.0,1.0,4.0,1.0,9.0,3.0,5.0,7.0,5.0,4.0,1.0,5.0,1.0,1.0,1.0,10.0,4.0,3.0,7.0,6.0,1.0,1.0,1.0,6.0,1.0,1.0,4.0,1.0,1.0,4.0,1.0,1.0,8.0,1.0,5.0,1.0,2.0,10.0,3.0,1.0,1.0,1.0,1.0,2.0,8.0,1.0,1.0,7.0,10.0,1.0,7.0,7.0,1.0,3.0,1.0,6.0,1.0,1.0,6.0,2.0,1.0,6.0,3.0,1.0,10.0,5.0,4.0,3.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
2020-01-25 15:50:40 ERROR ImputationPlan:388 â RMSE: 1.2358791446876887
2020-01-25 15:50:40 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 15:50:40 ERROR ImputationPlan:388 â avgPercentError: 1.3065248432106433
2020-01-25 15:50:40 ERROR ImputationPlan:388 â varianceCompleteError: 7.857055732782882
2020-01-25 15:50:40 ERROR ImputationPlan:388 â varianceImputedError: 7.76184255092661
2020-01-25 15:50:40 ERROR ImputationPlan:388 â weights: 4.624972813284271
2020-01-25 15:50:40 ERROR ImputationPlan:388 â varianceImputedErrorTotal: 7.5203602927378475
2020-01-25 15:50:40 ERROR ImputationPlan:388 â varianceCompleteErrorTotal: 7.857055732782884
2020-01-25 15:50:40 ERROR ImputationPlan:388 â RMSE: 1.2358791446876887
2020-01-25 15:50:40 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 15:50:40 ERROR ImputationPlan:388 â Total plan execution time: 354 seconds, 5 minutes, 0 hours.
2020-01-25 15:50:40 ERROR Crowner:103 â Executed plans: 3 / 9 : 34%.
2020-01-25 18:12:40 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 18:12:38
2020-01-25 18:12:40 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 18:12:46 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 18:24:02 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 18:24:02 ERROR ImputationPlan:388 â Running imputation plan: Clustering[KMeans]->Imputation[AdaboostR2]
2020-01-25 18:24:02 ERROR ImputationPlan:388 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 18:24:02 ERROR ImputationPlan:388 â Running Clustering[KMeans] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@757bbd02)
2020-01-25 18:24:02 ERROR ImputationPlan:388 â ClusteringResult: ClusteringResult(MapPartitionsRDD[122] at map at KMeans.scala:44,4,3178.2024580017683)
2020-01-25 18:24:02 ERROR ImputationPlan:388 â best k: 4
2020-01-25 18:24:02 ERROR ImputationPlan:388 â Batch for imputation after clustering strategy: 4
2020-01-25 18:24:02 ERROR ImputationPlan:388 â Batch for imputation before imputation strategy: 4
2020-01-25 18:24:02 ERROR ImputationPlan:388 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@4d33713a, calcFeatures -> [Ljava.lang.String;@757bbd02)
2020-01-25 18:24:02 ERROR ImputationPlan:388 â ImputationResult: 2.537561082271022,2.537561082271022,2.537561082271022,4.999999999999999,7.462438917728978,2.537561082271022,2.537561082271022,2.537561082271022,5.999999999999999,2.537561082271022,7.0,9.999999999999998,9.53756108227102,2.537561082271022,9.999999999999998,2.537561082271022,4.999999999999999,9.53756108227102,2.537561082271022,9.999999999999998,9.999999999999998,7.0,2.537561082271022,4.999999999999999,7.462438917728978,9.999999999999998,9.999999999999998,5.999999999999999,4.999999999999999
2020-01-25 18:24:02 ERROR ImputationPlan:388 â OriginalValues: 1.0,5.0,5.0,9.0,6.0,8.0,5.0,2.0,8.0,5.0,7.0,10.0,9.0,3.0,10.0,3.0,5.0,9.0,2.0,10.0,10.0,7.0,3.0,5.0,8.0,10.0,10.0,6.0,5.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â RMSE: 1.6679730744711443
2020-01-25 18:24:02 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â avgPercentError: 1.6884337424360227
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceCompleteError: 7.690844233055882
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceImputedError: 8.879639683945191
2020-01-25 18:24:02 ERROR ImputationPlan:388 â weights: 1.6739764335716716,1.9459101490553132
2020-01-25 18:24:02 ERROR ImputationPlan:388 â ImputationResult: 1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.9999999999999996,1.0,2.0,1.0,1.0,1.0,2.9999999999999996,1.0,1.0,1.0,2.0,1.0,1.0,1.0,4.0,2.9999999999999996,5.999999999999999,1.0,1.0,2.0,4.0,4.0,2.0,1.0,2.0,5.0,1.0,1.0,1.0,2.9999999999999996,1.0,2.9999999999999996,1.0,1.0,4.0,2.0,2.0,1.0,4.0,1.0,2.9999999999999996,4.0,1.0,1.0,5.0,2.0,1.0,2.9999999999999996,1.0,2.0,2.0,1.0,2.9999999999999996,2.9999999999999996,5.0,4.0,6.40239085244606,4.0,4.0,2.0,1.0,1.0,2.9999999999999996,2.9999999999999996,4.0,2.0,5.0,2.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â OriginalValues: 5.0,6.0,1.0,2.0,1.0,3.0,2.0,4.0,3.0,1.0,1.0,3.0,5.0,3.0,2.0,1.0,5.0,2.0,3.0,5.0,3.0,4.0,4.0,1.0,4.0,3.0,6.0,1.0,1.0,2.0,4.0,4.0,2.0,1.0,2.0,5.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,4.0,2.0,2.0,1.0,4.0,1.0,3.0,4.0,1.0,1.0,5.0,2.0,1.0,3.0,1.0,2.0,2.0,1.0,3.0,3.0,5.0,4.0,8.0,4.0,4.0,2.0,1.0,1.0,3.0,3.0,4.0,2.0,5.0,2.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â RMSE: 1.2687180840414045
2020-01-25 18:24:02 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â avgPercentError: 1.2136858472563237
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceCompleteError: 2.464332675871135
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceImputedError: 2.2527152955812886
2020-01-25 18:24:02 ERROR ImputationPlan:388 â weights: 3.970291913552122,1.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â ImputationResult: 8.630738704554814,10.0,10.0,7.89221611366444,10.0,9.0,8.630738704554814,9.0,10.0,10.0,7.522954818219254,8.630738704554814,10.0,10.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â OriginalValues: 8.0,10.0,6.0,6.0,10.0,9.0,8.0,9.0,10.0,10.0,5.0,8.0,10.0,10.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â RMSE: 1.392307354611083
2020-01-25 18:24:02 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â avgPercentError: 0.899929945796648
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceCompleteError: 2.8214285714285716
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceImputedError: 1.711948832504041
2020-01-25 18:24:02 ERROR ImputationPlan:388 â weights: 1.70810943994113,1.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â ImputationResult: 5.0,4.0,5.318464627155041,2.9999999999999996,5.0,5.0,8.0,5.0,2.9999999999999996,5.0,4.0,10.0,7.0,10.0,5.0,2.9999999999999996,5.318464627155041,7.0,8.318464627155041,10.0,5.0,8.0,7.0,2.9999999999999996,10.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â OriginalValues: 5.0,5.0,1.0,8.0,10.0,7.0,8.0,5.0,3.0,5.0,4.0,10.0,7.0,10.0,5.0,3.0,6.0,7.0,9.0,10.0,5.0,8.0,7.0,3.0,10.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â RMSE: 1.7271724600035419
2020-01-25 18:24:02 ERROR ImputationPlan:388 â avgError: 0.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â avgPercentError: 1.7224671376421499
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceCompleteError: 6.486400000000001
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceImputedError: 5.6376322107509775
2020-01-25 18:24:02 ERROR ImputationPlan:388 â weights: 2.1400661634962708,1.0
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceImputedErrorTotal: 9.52350525338076
2020-01-25 18:24:02 ERROR ImputationPlan:388 â varianceCompleteErrorTotal: 8.833974479264407
2020-01-25 18:24:02 ERROR ImputationPlan:388 â RMSE: 1.4523284105535206
2020-01-25 18:24:02 ERROR ImputationPlan:388 â -------------------------------------
2020-01-25 18:24:02 ERROR ImputationPlan:388 â Total plan execution time: 673 seconds, 11 minutes, 0 hours.
2020-01-25 18:24:02 ERROR Crowner:103 â Executed plans: 1 / 9 : 12%.
2020-01-25 18:36:21 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 18:36:19
2020-01-25 18:36:21 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 18:36:27 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 18:40:45 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 18:40:44
2020-01-25 18:40:45 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 18:40:50 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 18:41:38 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 1 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.probabilize(BoostingParams.scala:93)
	at org.apache.spark.ml.regression.BoostingRegressor.probabilize(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:229)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 18:41:38 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 1 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.probabilize(BoostingParams.scala:93)
	at org.apache.spark.ml.regression.BoostingRegressor.probabilize(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:229)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 18:41:38 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 18:41:38 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 18:41:38 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 18:41:38 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@446dea16)
2020-01-25 18:41:38 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[705] at map at KMeans.scala:44,15,1496.737662337663)
2020-01-25 18:41:38 ERROR ImputationPlan:371 â best k: 15
2020-01-25 18:41:38 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 15
2020-01-25 18:41:38 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 13
2020-01-25 18:41:38 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@446dea16)
2020-01-25 18:41:38 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 1 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.probabilize(BoostingParams.scala:93)
	at org.apache.spark.ml.regression.BoostingRegressor.probabilize(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:229)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 18:41:38 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 18:41:38 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 18:41:38 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 18:41:38 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@446dea16)
2020-01-25 18:41:38 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[705] at map at KMeans.scala:44,15,1496.737662337663)
2020-01-25 18:41:38 ERROR ImputationPlan:390 â best k: 15
2020-01-25 18:41:38 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 15
2020-01-25 18:41:38 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 13
2020-01-25 18:41:38 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@446dea16)
2020-01-25 18:41:38 ERROR ImputationPlan:390 â Total plan execution time: 45 seconds, 0 minutes, 0 hours.
2020-01-25 18:41:38 ERROR Crowner:103 â Executed plans: 1 / 9 : 12%.
2020-01-25 18:42:19 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 18:42:19 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 18:42:19 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 18:42:19 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 18:42:19 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 18:42:19 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature uniformity_of_cell_size
2020-01-25 18:42:19 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@4225893f)
2020-01-25 18:42:19 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[2217] at map at KMeans.scala:44,7,2709.954365079363)
2020-01-25 18:42:19 ERROR ImputationPlan:371 â best k: 7
2020-01-25 18:42:19 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 7
2020-01-25 18:42:19 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 6
2020-01-25 18:42:19 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> uniformity_of_cell_size, kLimit -> 10, varianceComplete -> 8.031713267029476, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@4225893f)
2020-01-25 18:42:19 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 18:42:19 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 18:42:19 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 18:42:19 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature uniformity_of_cell_size
2020-01-25 18:42:19 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@4225893f)
2020-01-25 18:42:19 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[2217] at map at KMeans.scala:44,7,2709.954365079363)
2020-01-25 18:42:19 ERROR ImputationPlan:390 â best k: 7
2020-01-25 18:42:19 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 7
2020-01-25 18:42:19 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 6
2020-01-25 18:42:19 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> uniformity_of_cell_size, kLimit -> 10, varianceComplete -> 8.031713267029476, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@4225893f)
2020-01-25 18:42:19 ERROR ImputationPlan:390 â Total plan execution time: 40 seconds, 0 minutes, 0 hours.
2020-01-25 18:42:19 ERROR Crowner:103 â Executed plans: 2 / 9 : 23%.
2020-01-25 18:47:06 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 18:47:02
2020-01-25 18:47:06 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 18:47:16 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 18:58:18 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 18:58:14
2020-01-25 18:58:18 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 18:58:29 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 19:01:42 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 19:01:42 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:01:42 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature uniformity_of_cell_shape
2020-01-25 19:01:42 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@5e2c751e)
2020-01-25 19:01:42 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[3293] at map at KMeans.scala:44,5,3096.0513547997184)
2020-01-25 19:01:42 ERROR ImputationPlan:390 â best k: 5
2020-01-25 19:01:42 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 5
2020-01-25 19:01:42 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 5
2020-01-25 19:01:42 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> uniformity_of_cell_shape, kLimit -> 10, varianceComplete -> 7.857055732782882, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@5e2c751e)
2020-01-25 19:01:42 ERROR ImputationPlan:390 â ImputationResult: 7.0,10.0,10.0,7.0,9.0,10.0,7.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â OriginalValues: 6.0,10.0,10.0,2.0,9.0,10.0,7.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â RMSE: 1.927248223318863
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgPercentError: 2.8574433989716765
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceCompleteError: 7.632653061224489
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceImputedError: 5.2727272727272725
2020-01-25 19:01:42 ERROR ImputationPlan:390 â weights: 1.0986122886681098
2020-01-25 19:01:42 ERROR ImputationPlan:390 â ImputationResult: 4.0,4.0,3.0,4.0,4.0,4.0,4.0,4.0,3.0,4.0,4.0,3.0,8.0,5.0,6.0,3.0,5.0,8.0,5.0,5.0,3.0,6.0,3.0,5.0,5.0,4.0,7.0,4.0,3.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â OriginalValues: 10.0,5.0,6.0,3.0,4.0,2.0,3.0,3.0,3.0,6.0,4.0,4.0,8.0,5.0,6.0,1.0,5.0,8.0,5.0,5.0,3.0,6.0,3.0,5.0,5.0,4.0,7.0,4.0,3.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â RMSE: 1.4621665549733924
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgPercentError: 1.4664101773807914
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceCompleteError: 3.6623067776456595
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceImputedError: 2.322482638888888
2020-01-25 19:01:42 ERROR ImputationPlan:390 â weights: 2.6532419646072154
2020-01-25 19:01:42 ERROR ImputationPlan:390 â ImputationResult: 1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â OriginalValues: 1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â RMSE: 0.4500351603704096
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgPercentError: 0.4146996692315224
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceCompleteError: 0.4540938952091009
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceImputedError: 0.31605831037649207
2020-01-25 19:01:42 ERROR ImputationPlan:390 â weights: 1.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â ImputationResult: 3.3207042086813607,3.3207042086813607,3.3207042086813607,8.18813567209113,10.0,10.0,7.776513283389515,7.776513283389515,10.0,3.3207042086813607,5.827525821524786,10.0,6.294188805649194
2020-01-25 19:01:42 ERROR ImputationPlan:390 â OriginalValues: 4.0,5.0,5.0,10.0,10.0,10.0,7.0,7.0,10.0,3.0,4.0,10.0,5.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â RMSE: 1.0992282630368475
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgPercentError: 1.4036428788096569
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceCompleteError: 6.994082840236687
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceImputedError: 7.2996563571049204
2020-01-25 19:01:42 ERROR ImputationPlan:390 â weights: 2.1972245773362196,1.773362104088339,1.8806629057590574,1.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â ImputationResult: 8.0,6.0,6.0,5.0,2.0,2.0,6.0,4.0,7.0,4.0,2.0,5.0,8.0,5.0,8.0,5.0,7.0,7.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â OriginalValues: 8.0,6.0,6.0,5.0,3.0,3.0,6.0,4.0,7.0,4.0,2.0,5.0,8.0,5.0,10.0,5.0,7.0,8.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â RMSE: 0.6236095644623236
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 19:01:42 ERROR ImputationPlan:390 â avgPercentError: NaN
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceCompleteError: 4.111111111111112
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceImputedError: 3.905306122448981
2020-01-25 19:01:42 ERROR ImputationPlan:390 â weights: 2.164963715117998
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceImputedErrorTotal: 7.344825256645082
2020-01-25 19:01:42 ERROR ImputationPlan:390 â varianceCompleteErrorTotal: 7.8570557327829
2020-01-25 19:01:42 ERROR ImputationPlan:390 â RMSE: 0.9315914210587093
2020-01-25 19:01:42 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 19:01:42 ERROR ImputationPlan:390 â Total plan execution time: 1162 seconds, 19 minutes, 0 hours.
2020-01-25 19:01:42 ERROR Crowner:103 â Executed plans: 3 / 9 : 34%.
2020-01-25 19:02:57 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 19:02:53
2020-01-25 19:02:57 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 19:03:10 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 19:03:57 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:03:57 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:03:57 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:03:57 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 19:03:57 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:03:57 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature marginal_adhesion
2020-01-25 19:03:57 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@93136e5)
2020-01-25 19:03:57 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[13112] at map at KMeans.scala:44,6,2667.0321032596576)
2020-01-25 19:03:57 ERROR ImputationPlan:371 â best k: 6
2020-01-25 19:03:57 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 6
2020-01-25 19:03:57 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 5
2020-01-25 19:03:57 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> marginal_adhesion, kLimit -> 10, varianceComplete -> 7.177190842559583, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@93136e5)
2020-01-25 19:03:57 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:03:57 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 19:03:57 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:03:57 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature marginal_adhesion
2020-01-25 19:03:57 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@93136e5)
2020-01-25 19:03:57 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[13112] at map at KMeans.scala:44,6,2667.0321032596576)
2020-01-25 19:03:57 ERROR ImputationPlan:390 â best k: 6
2020-01-25 19:03:57 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 6
2020-01-25 19:03:57 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 5
2020-01-25 19:03:57 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> marginal_adhesion, kLimit -> 10, varianceComplete -> 7.177190842559583, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@93136e5)
2020-01-25 19:03:57 ERROR ImputationPlan:390 â Total plan execution time: 134 seconds, 2 minutes, 0 hours.
2020-01-25 19:03:57 ERROR Crowner:103 â Executed plans: 4 / 9 : 45%.
2020-01-25 19:06:10 ERROR Instrumentation:70 â org.apache.spark.SparkException: Job 2432 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:06:10 ERROR Instrumentation:70 â org.apache.spark.SparkException: Job 2432 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:06:10 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: org.apache.spark.SparkException: Job 2432 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:06:10 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 19:06:10 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:06:10 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature single_epithelial_cell_size
2020-01-25 19:06:10 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@58a4c90c)
2020-01-25 19:06:10 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[14637] at map at KMeans.scala:44,8,2282.617265087852)
2020-01-25 19:06:10 ERROR ImputationPlan:371 â best k: 8
2020-01-25 19:06:10 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 8
2020-01-25 19:06:10 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 8
2020-01-25 19:06:10 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> single_epithelial_cell_size, kLimit -> 10, varianceComplete -> 7.14374179020453, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@58a4c90c)
2020-01-25 19:06:10 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: org.apache.spark.SparkException: Job 2432 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 19:06:10 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:06:10 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature single_epithelial_cell_size
2020-01-25 19:06:10 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@58a4c90c)
2020-01-25 19:06:10 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[14637] at map at KMeans.scala:44,8,2282.617265087852)
2020-01-25 19:06:10 ERROR ImputationPlan:390 â best k: 8
2020-01-25 19:06:10 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 8
2020-01-25 19:06:10 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 8
2020-01-25 19:06:10 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> single_epithelial_cell_size, kLimit -> 10, varianceComplete -> 7.14374179020453, features -> [Ljava.lang.String;@713fa9dc, calcFeatures -> [Ljava.lang.String;@58a4c90c)
2020-01-25 19:06:10 ERROR ImputationPlan:390 â Total plan execution time: 132 seconds, 2 minutes, 0 hours.
2020-01-25 19:06:10 ERROR Crowner:103 â Executed plans: 5 / 9 : 56%.
2020-01-25 19:06:10 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@51f11d8f rejected from java.util.concurrent.ThreadPoolExecutor@171dea50[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493844]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@163f983a rejected from java.util.concurrent.ThreadPoolExecutor@f17a394[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493852]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@20bc9ddd rejected from java.util.concurrent.ThreadPoolExecutor@171dea50[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493844]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@5eb164c5 rejected from java.util.concurrent.ThreadPoolExecutor@f17a394[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493852]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@2537a295 rejected from java.util.concurrent.ThreadPoolExecutor@171dea50[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493844]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@5503d6e9 rejected from java.util.concurrent.ThreadPoolExecutor@f17a394[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493852]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@5de30db4 rejected from java.util.concurrent.ThreadPoolExecutor@171dea50[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493844]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6191ba0f rejected from java.util.concurrent.ThreadPoolExecutor@f17a394[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493852]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@1b32ddbf rejected from java.util.concurrent.ThreadPoolExecutor@171dea50[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493844]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@23e1beb rejected from java.util.concurrent.ThreadPoolExecutor@f17a394[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493852]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@78ea3d3f rejected from java.util.concurrent.ThreadPoolExecutor@171dea50[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493844]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@4b82e0d2 rejected from java.util.concurrent.ThreadPoolExecutor@f17a394[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493852]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@3974369b rejected from java.util.concurrent.ThreadPoolExecutor@171dea50[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493844]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@5854beef rejected from java.util.concurrent.ThreadPoolExecutor@f17a394[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493852]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR TaskSchedulerImpl:91 â Exception in statusUpdate
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@38f27b06 rejected from java.util.concurrent.ThreadPoolExecutor@171dea50[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493844]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
	at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:558)
	at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:539)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:06:10 ERROR Inbox:91 â Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@1fc6b9b9 rejected from java.util.concurrent.ThreadPoolExecutor@f17a394[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 493852]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:206)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:88)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$reviveOffers$1.apply(LocalSchedulerBackend.scala:86)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:86)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:70)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:07:51 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 19:07:49
2020-01-25 19:07:51 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 19:07:57 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 19:24:43 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:24:43 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:24:43 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:24:43 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 19:24:43 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:24:43 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 19:24:43 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@446dea16)
2020-01-25 19:24:43 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[267] at map at KMeans.scala:44,8,2265.88794334321)
2020-01-25 19:24:43 ERROR ImputationPlan:371 â best k: 8
2020-01-25 19:24:43 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 8
2020-01-25 19:24:43 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 8
2020-01-25 19:24:43 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@1deb3f79, calcFeatures -> [Ljava.lang.String;@446dea16)
2020-01-25 19:24:43 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:24:43 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 19:24:43 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:24:43 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 19:24:43 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@446dea16)
2020-01-25 19:24:43 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[267] at map at KMeans.scala:44,8,2265.88794334321)
2020-01-25 19:24:43 ERROR ImputationPlan:390 â best k: 8
2020-01-25 19:24:43 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 8
2020-01-25 19:24:43 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 8
2020-01-25 19:24:43 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@1deb3f79, calcFeatures -> [Ljava.lang.String;@446dea16)
2020-01-25 19:24:43 ERROR ImputationPlan:390 â Total plan execution time: 1003 seconds, 16 minutes, 0 hours.
2020-01-25 19:24:43 ERROR Crowner:103 â Executed plans: 1 / 9 : 12%.
2020-01-25 19:46:55 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 19:46:53
2020-01-25 19:46:55 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 19:47:02 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 19:49:00 ERROR ImputationPlanAdaboostR2Exec$:57 â Appraisal Spark - Wall start time: 25/01/2020 19:48:59
2020-01-25 19:49:00 ERROR ImputationPlanAdaboostR2Exec$:58 â Parallel execution: false
2020-01-25 19:49:06 ERROR ImputationPlanAdaboostR2Exec$:104 â Data count: 146
2020-01-25 19:49:55 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.normL2(MultivariateOnlineSummarizer.scala:281)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)
	at org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:159)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 19:49:55 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 19:49:55 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:49:55 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 19:49:55 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@45fdd38d)
2020-01-25 19:49:55 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[273] at map at KMeans.scala:44,8,2349.275983436852)
2020-01-25 19:49:55 ERROR ImputationPlan:371 â best k: 8
2020-01-25 19:49:55 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 8
2020-01-25 19:49:55 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 7
2020-01-25 19:49:55 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@45fdd38d)
2020-01-25 19:49:55 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.normL2(MultivariateOnlineSummarizer.scala:281)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)
	at org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:159)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 19:49:55 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 19:49:55 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 19:49:55 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature clump_thickness
2020-01-25 19:49:55 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@45fdd38d)
2020-01-25 19:49:55 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[273] at map at KMeans.scala:44,8,2349.275983436852)
2020-01-25 19:49:55 ERROR ImputationPlan:390 â best k: 8
2020-01-25 19:49:55 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 8
2020-01-25 19:49:55 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 7
2020-01-25 19:49:55 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> clump_thickness, kLimit -> 10, varianceComplete -> 8.8339744792644, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@45fdd38d)
2020-01-25 19:49:55 ERROR ImputationPlan:390 â Total plan execution time: 46 seconds, 0 minutes, 0 hours.
2020-01-25 19:49:55 ERROR Crowner:103 â Executed plans: 1 / 9 : 12%.
2020-01-25 20:01:20 ERROR Instrumentation:70 â java.lang.NullPointerException: Value at index 1 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.probabilize(BoostingParams.scala:93)
	at org.apache.spark.ml.regression.BoostingRegressor.probabilize(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:229)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:01:20 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 1 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.probabilize(BoostingParams.scala:93)
	at org.apache.spark.ml.regression.BoostingRegressor.probabilize(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:229)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:01:20 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 20:01:20 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:01:20 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature uniformity_of_cell_size
2020-01-25 20:01:20 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@656be542)
2020-01-25 20:01:20 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[1596] at map at KMeans.scala:44,13,1921.4233727119056)
2020-01-25 20:01:20 ERROR ImputationPlan:371 â best k: 13
2020-01-25 20:01:20 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 13
2020-01-25 20:01:20 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 11
2020-01-25 20:01:20 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> uniformity_of_cell_size, kLimit -> 10, varianceComplete -> 8.031713267029476, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@656be542)
2020-01-25 20:01:20 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.NullPointerException: Value at index 1 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at org.apache.spark.ml.boosting.BoostingParams$class.probabilize(BoostingParams.scala:93)
	at org.apache.spark.ml.regression.BoostingRegressor.probabilize(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:229)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 20:01:20 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 20:01:20 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:01:20 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature uniformity_of_cell_size
2020-01-25 20:01:20 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@656be542)
2020-01-25 20:01:20 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[1596] at map at KMeans.scala:44,13,1921.4233727119056)
2020-01-25 20:01:20 ERROR ImputationPlan:390 â best k: 13
2020-01-25 20:01:20 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 13
2020-01-25 20:01:20 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 11
2020-01-25 20:01:20 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> uniformity_of_cell_size, kLimit -> 10, varianceComplete -> 8.031713267029476, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@656be542)
2020-01-25 20:01:20 ERROR ImputationPlan:390 â Total plan execution time: 685 seconds, 11 minutes, 0 hours.
2020-01-25 20:01:20 ERROR Crowner:103 â Executed plans: 2 / 9 : 23%.
2020-01-25 20:17:15 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 20:17:15 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:17:15 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature uniformity_of_cell_shape
2020-01-25 20:17:15 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@31d65c0e)
2020-01-25 20:17:15 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[12819] at map at KMeans.scala:44,7,2578.822064777327)
2020-01-25 20:17:15 ERROR ImputationPlan:390 â best k: 7
2020-01-25 20:17:15 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 7
2020-01-25 20:17:15 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 7
2020-01-25 20:17:15 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> uniformity_of_cell_shape, kLimit -> 10, varianceComplete -> 7.857055732782882, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@31d65c0e)
2020-01-25 20:17:15 ERROR ImputationPlan:390 â ImputationResult: 1.0,1.0,1.0,4.0,1.0,3.0,4.0,4.0,1.0,8.0,4.0,3.0,5.0,4.0,3.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â OriginalValues: 5.0,3.0,2.0,5.0,3.0,3.0,6.0,4.0,1.0,8.0,4.0,3.0,5.0,4.0,3.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â RMSE: 1.4142135623730951
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgPercentError: 2.281627282381159
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceCompleteError: 2.728888888888889
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceImputedError: 3.724007561436673
2020-01-25 20:17:15 ERROR ImputationPlan:390 â weights: 1.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â ImputationResult: 4.0,4.0,3.4173975867317066,4.0,4.0,4.747807239804879,3.582602413268293,3.582602413268293,3.582602413268293,3.4173975867317066,2.165204826536586,4.0,5.330409653073172
2020-01-25 20:17:15 ERROR ImputationPlan:390 â OriginalValues: 4.0,5.0,3.0,5.0,4.0,6.0,3.0,5.0,4.0,3.0,1.0,4.0,7.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â RMSE: 0.9023185796801373
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgPercentError: 1.595927332495334
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceCompleteError: 2.130177514792899
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceImputedError: 1.43025548864637
2020-01-25 20:17:15 ERROR ImputationPlan:390 â weights: 1.3862943611198906,1.9349858886649915
2020-01-25 20:17:15 ERROR ImputationPlan:390 â ImputationResult: 1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0000000000000004,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0000000000000004,2.0,1.0,3.0000000000000004,1.0,1.0,1.0,1.0,1.0,3.2019694559262195,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â OriginalValues: 1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â RMSE: 0.3913010139816764
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgPercentError: 0.46862258136693685
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceCompleteError: 0.3717105263157895
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceImputedError: 0.3308771878102478
2020-01-25 20:17:15 ERROR ImputationPlan:390 â weights: 3.9512437185814275,1.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â ImputationResult: 7.0,10.0,10.0,10.0,7.0,7.0,10.0,5.0,10.0,5.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â OriginalValues: 7.0,10.0,10.0,10.0,7.0,7.0,10.0,4.0,10.0,5.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â RMSE: 0.31622776601683794
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgPercentError: 1.8055214529759187
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceCompleteError: 4.8
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceImputedError: 4.6875
2020-01-25 20:17:15 ERROR ImputationPlan:390 â weights: 1.6094379124341003
2020-01-25 20:17:15 ERROR ImputationPlan:390 â ImputationResult: 10.0,10.0,5.27507112680287,10.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â OriginalValues: 10.0,10.0,2.0,10.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â RMSE: 1.637535563401435
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgPercentError: 1.885618083164127
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceCompleteError: 12.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceImputedError: 9.027568472846475
2020-01-25 20:17:15 ERROR ImputationPlan:390 â weights: 0.6931471805599455,0.0,1.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â ImputationResult: 6.865673353850133,6.865673353850133,5.999999999999999,5.999999999999999,5.0,3.865673353850134,3.4328366769250667,5.999999999999999,4.0,6.432836676925067,9.134326646149866,5.0,9.134326646149866,5.0,6.432836676925067
2020-01-25 20:17:15 ERROR ImputationPlan:390 â OriginalValues: 5.0,2.0,6.0,6.0,5.0,3.0,3.0,6.0,4.0,7.0,8.0,5.0,10.0,5.0,8.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â RMSE: 1.4811260267345177
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgPercentError: 1.5491896250995405
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceCompleteError: 4.248888888888889
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceImputedError: 3.192141127136503
2020-01-25 20:17:15 ERROR ImputationPlan:390 â weights: 1.4350845252893227,1.880449027653477
2020-01-25 20:17:15 ERROR ImputationPlan:390 â ImputationResult: 7.525789285683649,5.0,6.515473571410189,6.989684285726541,6.515473571410189,2.979368571453081,9.01031571427346,5.0,5.50515785713673,6.515473571410189,8.0,7.000000000000001,7.525789285683649
2020-01-25 20:17:15 ERROR ImputationPlan:390 â OriginalValues: 9.0,4.0,5.0,8.0,6.0,1.0,10.0,5.0,6.0,5.0,8.0,7.0,6.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â RMSE: 1.1273981272262283
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:17:15 ERROR ImputationPlan:390 â avgPercentError: NaN
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceCompleteError: 5.053254437869823
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceImputedError: 3.52275867647966
2020-01-25 20:17:15 ERROR ImputationPlan:390 â weights: 1.7844867098989752,1.8216869671721112
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceImputedErrorTotal: 7.7675586740830855
2020-01-25 20:17:15 ERROR ImputationPlan:390 â varianceCompleteErrorTotal: 7.857055732782892
2020-01-25 20:17:15 ERROR ImputationPlan:390 â RMSE: 0.8812220184057284
2020-01-25 20:17:15 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 20:17:15 ERROR ImputationPlan:390 â Total plan execution time: 954 seconds, 15 minutes, 0 hours.
2020-01-25 20:17:15 ERROR Crowner:103 â Executed plans: 3 / 9 : 34%.
2020-01-25 20:34:12 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 20:34:12 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:34:12 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature marginal_adhesion
2020-01-25 20:34:12 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@5d26a8b6)
2020-01-25 20:34:12 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[26069] at map at KMeans.scala:44,7,2460.677298997852)
2020-01-25 20:34:12 ERROR ImputationPlan:390 â best k: 7
2020-01-25 20:34:12 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 7
2020-01-25 20:34:12 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 6
2020-01-25 20:34:12 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> marginal_adhesion, kLimit -> 10, varianceComplete -> 7.177190842559583, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@5d26a8b6)
2020-01-25 20:34:12 ERROR ImputationPlan:390 â ImputationResult: 2.0,2.0,4.0,2.0,4.0,2.0,1.0,1.0,1.0,1.0,4.0,3.0,1.0,4.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â OriginalValues: 1.0,4.0,6.0,1.0,4.0,2.0,1.0,1.0,1.0,1.0,4.0,3.0,1.0,4.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â RMSE: 0.8451542547285167
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgPercentError: 1.1021440802042837
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceCompleteError: 2.6734693877551017
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceImputedError: 1.6041666666666667
2020-01-25 20:34:12 ERROR ImputationPlan:390 â weights: 1.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â ImputationResult: 2.8188845148047847,5.106249300113739,2.9146039725762964,2.0844512955821934,3.147225820635947,6.987604489227709,1.2599156907577387,4.196018383736734,2.8188845148047847,1.6271834605103257,1.7560588296097397,6.101936537287187
2020-01-25 20:34:12 ERROR ImputationPlan:390 â OriginalValues: 2.0,1.0,3.0,1.0,4.0,7.0,1.0,5.0,3.0,2.0,1.0,8.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â RMSE: 1.4289438747416672
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgPercentError: 2.986862324485513
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceCompleteError: 5.305555555555556
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceImputedError: 4.43768369701864
2020-01-25 20:34:12 ERROR ImputationPlan:390 â weights: 1.252762968495368,1.2992829841302607,3.044522437723423,3.5074665752561995,2.1489774915210997,1.8348486492935465
2020-01-25 20:34:12 ERROR ImputationPlan:390 â ImputationResult: 10.0,1.0,5.0,5.0,10.0,10.0,1.0,1.0,1.0,10.0,1.0,10.0,5.0,1.0,5.0,5.0,5.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â OriginalValues: 10.0,1.0,9.0,3.0,3.0,4.0,1.0,4.0,1.0,10.0,4.0,10.0,2.0,1.0,5.0,4.0,10.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â RMSE: 3.0486255340823387
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgPercentError: 3.4578425768798957
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceCompleteError: 11.792387543252593
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceImputedError: 12.29964328180737
2020-01-25 20:34:12 ERROR ImputationPlan:390 â weights: 1.3490940774338
2020-01-25 20:34:12 ERROR ImputationPlan:390 â ImputationResult: 10.0,10.0,10.0,6.0,10.0,10.0,8.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â OriginalValues: 4.0,3.0,10.0,6.0,10.0,7.0,8.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â RMSE: 3.6645015252516173
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgPercentError: 3.050192745118995
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceCompleteError: 6.408163265306123
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceImputedError: 2.5950413223140494
2020-01-25 20:34:12 ERROR ImputationPlan:390 â weights: 1.0986122886681098
2020-01-25 20:34:12 ERROR ImputationPlan:390 â ImputationResult: 8.226917364591829,7.053771608127132,7.1134586822959145,8.226917364591829,1.5865728782323483,3.0,6.526885804063566,3.0,7.1134586822959145,8.226917364591829,6.526885804063566
2020-01-25 20:34:12 ERROR ImputationPlan:390 â OriginalValues: 10.0,8.0,2.0,2.0,1.0,3.0,6.0,3.0,8.0,10.0,6.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â RMSE: 2.5900191706044238
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgPercentError: 3.391703000796316
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceCompleteError: 10.049586776859504
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceImputedError: 6.579242854838434
2020-01-25 20:34:12 ERROR ImputationPlan:390 â weights: 1.791759469228055,1.6094379124341005,2.70805020110221
2020-01-25 20:34:12 ERROR ImputationPlan:390 â ImputationResult: 2.0,1.0,2.0,2.0,2.0,2.0,2.0,2.0,3.2004416110908718,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.2004416110908718,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â OriginalValues: 1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â bestK: 0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â RMSE: 0.5286758438157989
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgError: 0.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â avgPercentError: 0.606463940715231
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceCompleteError: 0.3278320781925972
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceImputedError: 0.3766536832775777
2020-01-25 20:34:12 ERROR ImputationPlan:390 â weights: 3.9889840465642745,1.0
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceImputedErrorTotal: 7.230921415638033
2020-01-25 20:34:12 ERROR ImputationPlan:390 â varianceCompleteErrorTotal: 6.6467346938775504
2020-01-25 20:34:12 ERROR ImputationPlan:390 â RMSE: 1.652645310641477
2020-01-25 20:34:12 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 20:34:12 ERROR ImputationPlan:390 â Total plan execution time: 1015 seconds, 16 minutes, 0 hours.
2020-01-25 20:34:12 ERROR Crowner:103 â Executed plans: 4 / 9 : 45%.
2020-01-25 20:37:24 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:37:24 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:37:24 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:37:24 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 20:37:24 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:37:24 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature single_epithelial_cell_size
2020-01-25 20:37:24 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@51808f7a)
2020-01-25 20:37:24 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[38451] at map at KMeans.scala:44,11,1897.722342331584)
2020-01-25 20:37:24 ERROR ImputationPlan:371 â best k: 11
2020-01-25 20:37:24 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 11
2020-01-25 20:37:24 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 11
2020-01-25 20:37:24 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> single_epithelial_cell_size, kLimit -> 10, varianceComplete -> 7.14374179020453, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@51808f7a)
2020-01-25 20:37:24 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 20:37:24 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 20:37:24 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:37:24 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature single_epithelial_cell_size
2020-01-25 20:37:24 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@51808f7a)
2020-01-25 20:37:24 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[38451] at map at KMeans.scala:44,11,1897.722342331584)
2020-01-25 20:37:24 ERROR ImputationPlan:390 â best k: 11
2020-01-25 20:37:24 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 11
2020-01-25 20:37:24 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 11
2020-01-25 20:37:24 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> single_epithelial_cell_size, kLimit -> 10, varianceComplete -> 7.14374179020453, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@51808f7a)
2020-01-25 20:37:24 ERROR ImputationPlan:390 â Total plan execution time: 192 seconds, 3 minutes, 0 hours.
2020-01-25 20:37:24 ERROR Crowner:103 â Executed plans: 5 / 9 : 56%.
2020-01-25 20:48:13 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:48:13 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:48:13 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:48:13 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 20:48:13 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:48:13 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature bare_nuclei
2020-01-25 20:48:13 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@5326023b)
2020-01-25 20:48:13 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[41864] at map at KMeans.scala:44,10,2068.749442299444)
2020-01-25 20:48:13 ERROR ImputationPlan:371 â best k: 10
2020-01-25 20:48:13 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 10
2020-01-25 20:48:13 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 9
2020-01-25 20:48:13 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> bare_nuclei, kLimit -> 10, varianceComplete -> 12.235738412460126, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@5326023b)
2020-01-25 20:48:13 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 20:48:13 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 20:48:13 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:48:13 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature bare_nuclei
2020-01-25 20:48:13 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@5326023b)
2020-01-25 20:48:13 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[41864] at map at KMeans.scala:44,10,2068.749442299444)
2020-01-25 20:48:13 ERROR ImputationPlan:390 â best k: 10
2020-01-25 20:48:13 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 10
2020-01-25 20:48:13 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 9
2020-01-25 20:48:13 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> bare_nuclei, kLimit -> 10, varianceComplete -> 12.235738412460126, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@5326023b)
2020-01-25 20:48:13 ERROR ImputationPlan:390 â Total plan execution time: 648 seconds, 10 minutes, 0 hours.
2020-01-25 20:48:13 ERROR Crowner:103 â Executed plans: 6 / 9 : 67%.
2020-01-25 20:58:24 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:58:24 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:58:24 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 20:58:24 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 20:58:24 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:58:24 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature bland_chromatin
2020-01-25 20:58:24 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@7f90f795)
2020-01-25 20:58:24 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[51685] at map at KMeans.scala:44,9,2265.8582917082913)
2020-01-25 20:58:24 ERROR ImputationPlan:371 â best k: 9
2020-01-25 20:58:24 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 9
2020-01-25 20:58:24 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 9
2020-01-25 20:58:24 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> bland_chromatin, kLimit -> 10, varianceComplete -> 3.9878025896040548, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@7f90f795)
2020-01-25 20:58:24 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 20:58:24 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 20:58:24 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 20:58:24 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature bland_chromatin
2020-01-25 20:58:24 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@7f90f795)
2020-01-25 20:58:24 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[51685] at map at KMeans.scala:44,9,2265.8582917082913)
2020-01-25 20:58:24 ERROR ImputationPlan:390 â best k: 9
2020-01-25 20:58:24 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 9
2020-01-25 20:58:24 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 9
2020-01-25 20:58:24 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> bland_chromatin, kLimit -> 10, varianceComplete -> 3.9878025896040548, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@7f90f795)
2020-01-25 20:58:24 ERROR ImputationPlan:390 â Total plan execution time: 610 seconds, 10 minutes, 0 hours.
2020-01-25 20:58:24 ERROR Crowner:103 â Executed plans: 7 / 9 : 78%.
2020-01-25 21:08:12 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 21:08:12 ERROR Instrumentation:70 â java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 21:08:12 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 21:08:12 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 21:08:12 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 21:08:12 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature normal_nucleoli
2020-01-25 21:08:12 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@4abc4e7d)
2020-01-25 21:08:12 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[59761] at map at KMeans.scala:44,6,2503.8347398030955)
2020-01-25 21:08:12 ERROR ImputationPlan:371 â best k: 6
2020-01-25 21:08:12 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 6
2020-01-25 21:08:12 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 6
2020-01-25 21:08:12 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> normal_nucleoli, kLimit -> 10, varianceComplete -> 9.0838806530306, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@4abc4e7d)
2020-01-25 21:08:12 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: DecisionTree requires size of input RDD > 0, but was given by empty one.
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$$anonfun$8.apply(DecisionTreeMetadata.scala:113)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)
	at org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)
	at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.ensemble.HasBaseLearner$class.fitBaseLearner(ensembleParams.scala:115)
	at org.apache.spark.ml.regression.BoostingRegressor.fitBaseLearner(BoostingRegressor.scala:114)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.trainBoosters$1(BoostingRegressor.scala:238)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:328)
	at org.apache.spark.ml.regression.BoostingRegressor$$anonfun$train$1.apply(BoostingRegressor.scala:151)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:150)
	at org.apache.spark.ml.regression.BoostingRegressor.train(BoostingRegressor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
	at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:154)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 21:08:12 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 21:08:12 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 21:08:12 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature normal_nucleoli
2020-01-25 21:08:12 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@4abc4e7d)
2020-01-25 21:08:12 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[59761] at map at KMeans.scala:44,6,2503.8347398030955)
2020-01-25 21:08:12 ERROR ImputationPlan:390 â best k: 6
2020-01-25 21:08:12 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 6
2020-01-25 21:08:12 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 6
2020-01-25 21:08:12 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> normal_nucleoli, kLimit -> 10, varianceComplete -> 9.0838806530306, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@4abc4e7d)
2020-01-25 21:08:12 ERROR ImputationPlan:390 â Total plan execution time: 587 seconds, 9 minutes, 0 hours.
2020-01-25 21:08:12 ERROR Crowner:103 â Executed plans: 8 / 9 : 89%.
2020-01-25 21:10:32 ERROR Instrumentation:70 â org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.normL2(MultivariateOnlineSummarizer.scala:281)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)
	at org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:159)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-01-25 21:10:32 ERROR ImputationPlan:371 â -------------------------------------
2020-01-25 21:10:32 ERROR ImputationPlan:371 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 21:10:32 ERROR ImputationPlan:371 â Missing rate at 30.0% in feature mitoses
2020-01-25 21:10:32 ERROR ImputationPlan:371 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@5ac1c00f)
2020-01-25 21:10:32 ERROR ImputationPlan:371 â ClusteringResult: ClusteringResult(MapPartitionsRDD[66181] at map at KMeans.scala:44,14,1752.479790823212)
2020-01-25 21:10:32 ERROR ImputationPlan:371 â best k: 14
2020-01-25 21:10:32 ERROR ImputationPlan:371 â Batch for imputation after clustering strategy: 14
2020-01-25 21:10:32 ERROR ImputationPlan:371 â Batch for imputation before imputation strategy: 13
2020-01-25 21:10:32 ERROR ImputationPlan:371 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> mitoses, kLimit -> 10, varianceComplete -> 4.283965096641024, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@5ac1c00f)
2020-01-25 21:10:32 ERROR ImputationPlan:372 â Error executing imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$6.apply(CrossValidator.scala:166)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:166)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4.apply(CrossValidator.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:146)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:122)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:122)
	at appraisal.spark.algorithm.AdaboostR2.run(AdaboostR2.scala:105)
	at appraisal.spark.strategies.ImputationStrategy.run(ImputationStrategy.scala:12)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:319)
	at appraisal.spark.engine.ImputationPlan$$anonfun$13.apply(ImputationPlan.scala:314)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:314)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:104)
	at appraisal.spark.algorithm.Boost$$anonfun$run$3.apply(Boost.scala:102)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.algorithm.Boost.run(Boost.scala:102)
	at appraisal.spark.strategies.EnsembleStrategy.run(EnsembleStrategy.scala:11)
	at appraisal.spark.engine.ImputationPlan.run(ImputationPlan.scala:396)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:91)
	at appraisal.spark.engine.Crowner$$anonfun$runEnsemle$2.apply(Crowner.scala:89)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at appraisal.spark.engine.Crowner.runEnsemle(Crowner.scala:89)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec$.main(ImputationPlanAdaboostR2Exec.scala:201)
	at appraisal.spark.executor.poc.ImputationPlanAdaboostR2Exec.main(ImputationPlanAdaboostR2Exec.scala)
Caused by: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.normL2(MultivariateOnlineSummarizer.scala:281)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)
	at org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)
	at org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply$mcD$sp(CrossValidator.scala:159)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1$$anonfun$4$$anonfun$5$$anonfun$apply$1.apply(CrossValidator.scala:153)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-01-25 21:10:32 ERROR ImputationPlan:390 â -------------------------------------
2020-01-25 21:10:32 ERROR ImputationPlan:390 â Running imputation plan: Clustering[KMeansPlus]->Imputation[AdaboostR2]
2020-01-25 21:10:32 ERROR ImputationPlan:390 â Missing rate at 30.0% in feature mitoses
2020-01-25 21:10:32 ERROR ImputationPlan:390 â Running Clustering[KMeansPlus] | params: Map(k -> 4, kLimit -> 10, maxIter -> 1000, calcFeatures -> [Ljava.lang.String;@5ac1c00f)
2020-01-25 21:10:32 ERROR ImputationPlan:390 â ClusteringResult: ClusteringResult(MapPartitionsRDD[66181] at map at KMeans.scala:44,14,1752.479790823212)
2020-01-25 21:10:32 ERROR ImputationPlan:390 â best k: 14
2020-01-25 21:10:32 ERROR ImputationPlan:390 â Batch for imputation after clustering strategy: 14
2020-01-25 21:10:32 ERROR ImputationPlan:390 â Batch for imputation before imputation strategy: 13
2020-01-25 21:10:32 ERROR ImputationPlan:390 â Running Imputation[AdaboostR2] | params: Map(learningRate -> 0.1, k -> 5, imputationFeature -> mitoses, kLimit -> 10, varianceComplete -> 4.283965096641024, features -> [Ljava.lang.String;@664724e7, calcFeatures -> [Ljava.lang.String;@5ac1c00f)
2020-01-25 21:10:32 ERROR ImputationPlan:390 â Total plan execution time: 139 seconds, 2 minutes, 0 hours.
2020-01-25 21:10:32 ERROR Crowner:103 â Executed plans: 9 / 9 : 100%.
